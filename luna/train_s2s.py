# Luna ì‚¬ì „í•™ìŠµ + ë¯¸ì„¸ì¡°ì • (Gradient Checkpointing + FP16 + DeepSpeed ZeRO)

import os, sys, csv
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
import numpy as np
from tqdm import tqdm

drive_path = "/workspace"
# Colab í™˜ê²½ì—ì„œëŠ” drive_pathë¡œ ì´ë™ì´ í•„ìš” -> ì•„ë˜ì˜ í´ë˜ìŠ¤í‹€ì„ import í•˜ê¸° ìœ„í•¨
#%cd "$drive_path"

from transformer.luna.model import LunaTransformerEncoder, LunaTransformer
from transformer.code_transformer.dataset import SpellingDataset
from transformer.code_transformer.sentencepiece_tokenizer import SentencePieceTokenizer

# ------------------------
# ì•ˆì „í•œ ë””ì½”ë”© í•¨ìˆ˜
# ------------------------
def safe_decode(tokenizer, ids):
    try:
        # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ID í•„í„°ë§
        vocab_size = tokenizer.get_piece_size()
        valid_ids = [id for id in ids if id < vocab_size]

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if not valid_ids:
            return "<ë¹ˆ ì‹œí€€ìŠ¤>"

        return tokenizer.decode(valid_ids)
    except Exception as e:
        return f"ë””ì½”ë”© ì˜¤ë¥˜: {e}"

# ------------------------
# ê°€ì¥ ìµœì‹ ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ path ë°˜í™˜
# ------------------------
"""
    train, evaluate í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–‘ì‹ì„ ê·¸ëŒ€ë¡œ í•¨ìˆ˜ë¡œ ì˜®ê²¨ì„œ ì •ì˜
"""
def get_latest_checkpoint(checkpoint_dir):
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
    if not checkpoints:
        raise FileNotFoundError("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
    return os.path.join(checkpoint_dir, latest_checkpoint)

# ------------------------
# 1. Luna ëª¨ë¸ ì •ì˜
# ------------------------
"""
    ì œê±° ë˜ëŠ” model.pyë¡œ ì´ë™
"""
class LunaModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_attention_heads=8):
        super().__init__()
        self.encoder = LunaTransformerEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            num_layers=num_layers,
            num_attention_heads=num_attention_heads,
            d_ff=2048,
            dropout_p=0.2,
            project_embedding_length=32,
            max_length=1024
        )
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, input_lengths):
        # ì¸ì½”ë”ë¥¼ í†µê³¼
        encoder_outputs = self.encoder(input_ids, input_lengths)
        # ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ í†µê³¼
        logits = self.lm_head(encoder_outputs)
        return logits

# ------------------------
# 2. í•™ìŠµ ì„¤ì •
# ------------------------
"""
    ì£¼ì„ ì¶”ê°€ í•„ìš”
"""
def label_smoothed_loss(pred, target, epsilon=0.1, ignore_index=0, class_weight=None):
    """
    pred: (B*T, V)  - ë¡œì§“
    target: (B*T,)  - ì •ë‹µ í† í° id
    epsilon: ë¼ë²¨ ìŠ¤ë¬´ë”© ê³„ìˆ˜
    ignore_index: PAD id (ì˜ˆ: 0)
    class_weight: (V,) or None  - EOS ë“±ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ì‹œ ì‚¬ìš©
    """
    V = pred.size(-1)
    log_probs = F.log_softmax(pred, dim=-1)          # (N, V)

    # ìœ íš¨ í† í° ë§ˆìŠ¤í¬
    mask = (target != ignore_index)                  # (N,)
    if mask.sum() == 0:
        return pred.new_tensor(0.0)

    target_clamped = target.clone()
    target_clamped[~mask] = 0                        # one_hotì˜ ì¸ë±ìŠ¤ ì•ˆì „í™”

    one_hot = F.one_hot(target_clamped, num_classes=V).float()  # (N, V)
    # ë¼ë²¨ ìŠ¤ë¬´ë”©
    smoothed = (1 - epsilon) * one_hot + (epsilon / V)

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì˜ˆ: EOS 1.5)
    if class_weight is not None:
        # (V,) -> (N,V) ë¸Œë¡œë“œìºìŠ¤íŠ¸
        smoothed = smoothed * class_weight.unsqueeze(0)

    # í† í°ë³„ ì†ì‹¤
    loss_per_tok = -(smoothed * log_probs).sum(dim=-1)          # (N,)
    loss = loss_per_tok[mask].mean()
    return loss

# --- ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ìš© ---
import difflib

def _strip_special(ids, BOS, EOS, PAD):
    if torch.is_tensor(ids):
        ids = ids.tolist()
    if ids and ids[0] == BOS:
        ids = ids[1:]
    if EOS in ids:
        ids = ids[:ids.index(EOS)]
    return [t for t in ids if t != PAD]

def edit_counts_via_alignment(src_tokens, tgt_tokens, hyp_tokens):
    """
    srcâ†’tgt, srcâ†’hyp ë³€í™˜ì—ì„œ ë¹„-ë™ì¼ êµ¬ê°„ì„ í¸ì§‘ìœ¼ë¡œ ë³´ê³ ,
    src êµ¬ê°„ ê²¹ì¹¨ìœ¼ë¡œ TPë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ ì…‰ë‹ˆë‹¤. (ë¹ ë¥¸ ëª¨ë‹ˆí„°ë§ìš©)
    """
    sm_gold = difflib.SequenceMatcher(a=src_tokens, b=tgt_tokens)
    gold_spans = [op for op in sm_gold.get_opcodes() if op[0] != 'equal']

    sm_pred = difflib.SequenceMatcher(a=src_tokens, b=hyp_tokens)
    pred_spans = [op for op in sm_pred.get_opcodes() if op[0] != 'equal']

    tp = 0
    for tag, gi1, gi2, gj1, gj2 in gold_spans:
        for tag2, pi1, pi2, pj1, pj2 in pred_spans:
            # src ê¸°ì¤€ êµ¬ê°„ì´ ê²¹ì¹˜ë©´ TPë¡œ ì¹´ìš´íŠ¸
            if not (pi2 <= gi1 or pi1 >= gi2):
                tp += 1
                break

    fp = max(0, len(pred_spans) - tp)
    fn = max(0, len(gold_spans) - tp)
    return tp, fp, fn

"""
    train, evaluate í•¨ìˆ˜ëŠ” í•˜ë‚˜ë¡œ ë‘ê³ , íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ ì œì–´í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •.
        - train, evaluateì— íŒŒë¼ë¯¸í„° model_type ì¶”ê°€
        - model_typeì— ëŒ€í•œ ì‚¬ì „ê°’ì€ __init__ì—ì„œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¡œ ì •ì˜
"""
class pNup_s2s:
    def __init__(self):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        self.BATCH_SIZE = 32
        self.EPOCHS = 50
        self.LEARNING_RATE = 0.0001
        self.D_MODEL = 512
        self.NUM_HEADS = 8
        self.NUM_LAYERS = 6
        self.D_FF = 2048
        self.MAX_SEQ_LENGTH = 128
        self.DROPOUT = 0.2
        self.VOCAB_SIZE = 16000
        self.PAD_TOKEN_ID = 0    # íŒ¨ë”© í† í° ID
        self.BOS_TOKEN_ID = 2    # ì‹œì‘ í† í° ID( <s> or [BOS] )
        self.EOS_TOKEN_ID = 3    # ì¢…ë£Œ í† í° ID

    def train(self):
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •
        """ 
            í•´ë‹¹ ë³€ìˆ˜ë“¤ì€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë³€ê²½ì´ ìˆì§€ ì•ŠëŠ” í•œ ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë  ê²ƒì„. 
                -> __init__ì—ì„œ ì •ì˜ ë˜ëŠ” ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜
        """
        transformer_path = "/workspace/transformer"
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')
        tokenizer = SentencePieceTokenizer(train_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        train_loader = DataLoader(dataset.train_dataset, batch_size=self.BATCH_SIZE, shuffle=True)

        checkpoints = [f for f in os.listdir(f"{transformer_path}/checkpoints") if f.endswith('.pt')]
        
        # ëª¨ë¸ ê°ì²´ ìƒì„±
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=self.D_MODEL,
            num_layers=self.NUM_LAYERS,
            num_attention_heads=self.NUM_HEADS,
            d_ff = self.D_FF,
            dropout_p = self.DROPOUT,
            project_embedding_length = 32,
            max_length = self.MAX_SEQ_LENGTH
        )
        model = model.to(device)

        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.LEARNING_RATE, weight_decay=0.01)
        scaler = GradScaler()  # FP16ì„ ìœ„í•œ Gradient Scaler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=2
        )

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        """ $ì¶”ê°€. íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ ì¶”ê°€í•  ê²ƒ """
        if not checkpoints:
            print("ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ.")
            latest_checknum = 0
        else:
            # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint_path = get_latest_checkpoint(f"{transformer_path}/checkpoints")
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

            # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
            latest_checkpoint = os.path.basename(checkpoint_path)
            latest_checknum = int(latest_checkpoint.split('_')[-1].split('.')[0])

            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            model = model.to(device)

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (EOS í† í°ì— 1.5ë°° ê°€ì¤‘ì¹˜ ë¶€ì—¬)
        eos_weight = 1.5
        class_weight = torch.ones(self.VOCAB_SIZE, device=device)
        class_weight[self.EOS_TOKEN_ID] = eos_weight

        # í•™ìŠµ ë£¨í”„
        model.train()
        for epoch in range(latest_checknum, self.EPOCHS + latest_checknum):        
            epoch_gold_edit_tok = 0
            epoch_pred_edit_tok = 0
            epoch_nonpad_tok    = 0

            epoch_align_tp = 0
            epoch_align_fp = 0
            epoch_align_fn = 0
            epoch_align_calls = 0

            total_gold_edits = 0
            total_pred_edits = 0

            correct_edit_total = 0
            correct_tokens = 0
            total_tokens = 0

            total_loss_tok = 0
            epoch_loss_tok = 0

            LOG_ALIGN_EVERY = 500
            
            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.EPOCHS+latest_checknum}')

            for batch_idx, batch in enumerate(progress_bar):
                """
                    1. Beam Search ì¶”ê°€.
                    2. ê¸¸ì´ ì •ê·œí™” ë° íŒ¨ë„í‹° ì¶”ê°€.
                    3. ë°˜ë³µ íŒ¨ë„í‹° ìˆ˜ì •(ì´ˆì•ˆì—ì„œëŠ” ì ìš©X. í•„ìš”ì‹œ ì¶”ê°€)
                    4. ìŠ¤ì¼€ì¤„ë“œ ìƒ˜í”Œë§ ìˆ˜ì • ë° ë‚´ìš© ì •ë¦¬(3ê³¼ ì—°ê³„)
                    5. í‰ê°€ ë°©ì‹ ë³€ê²½(ìœ„ì¹˜ë³„ ì •í™•ë„ â†’ í¸ì§‘ ì •í™•ë„ : ì •ë ¬ê¸°ë°˜(Levenshtein/ERRANT)ìœ¼ë¡œ í¸ì§‘ì„ ë¹„êµí•˜ì—¬ P/R/F0.5 ì‚°ì¶œ)
                    6. ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒì„ ìœ„í•´ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ì ìš©.
                        - ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜í™”
                        - í•´ë‹¹ ë¶€ë¶„ì€ ìˆ˜ì • ë¼ì¸ ë°–ì˜ ë¶€ë¶„ë„ ë™ì¼í•˜ê²Œ ì ìš©
                    7. ë””ë²„ê¹…ìš© ì¶œë ¥ë¬¸ ì •ë¦¬
                        - ì¤‘ê°„ë‹¨ê³„ì—ì„œì˜ ì¶œë ¥ì´ í•„ìˆ˜ì ì´ì§€ ì•Šìœ¼ë©´ ì œê±° ë˜ëŠ” í›„ë°©ìœ¼ë¡œ ì´ë™
                    8. ì£¼ì„ ì •ë¦¬
                        - í•¨ìˆ˜í™” í•  ê²½ìš°, íŒŒë¼ë¯¸í„°ë¥¼ ëª…í™•íˆ ì£¼ì„ìœ¼ë¡œ í‘œì‹œ
                """
                # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
                input_ids   = batch['input_ids'].to(device)
                output_ids  = batch['output_ids'].to(device)
                
                # input_lengths ê³„ì‚° (íŒ¨ë”© í† í° 0ì„ ì œì™¸í•œ ì‹¤ì œ ê¸¸ì´)
                input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1).to(device)

                decoder_input   = output_ids[:, :-1]    # Decoder ì…ë ¥
                target          = output_ids[:, 1:]     # ì •ë‹µ
                optimizer.zero_grad()

                # ---- 1ì°¨ forward: teacher-forcingìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜ì§‘ ----
                with torch.no_grad():
                    logits_tf = model(input_ids, input_lengths, decoder_input)  # (B, T-1, V)
                    next_pred = logits_tf.argmax(dim=-1)                        # (B, T-1)

                # ---- ìŠ¤ì¼€ì¤„ë“œ ìƒ˜í”Œë§ í™•ë¥ : ì—í¬í¬ì— ë”°ë¼ ì ì§„ ì¦ê°€ ----
                max_ss = 0.25  # ìµœëŒ“ê°’(0.1~0.25 ê¶Œì¥)
                # latest_checknumì´ ìˆëŠ” ì½”ë“œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•´ ì§„í–‰ë¥  ê³„ì‚°
                ss_prob = min(max_ss, ((epoch - latest_checknum + 1) / self.EPOCHS) * max_ss)

                # ---- ì¼ë¶€ í† í°ì„ ëª¨ë¸ ì˜ˆì¸¡ìœ¼ë¡œ ì¹˜í™˜(ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ì¼ê´€ì„±) ----
                if ss_prob > 0.0:
                    # bern: (B, T-1)ì—ì„œ Trueì¸ ìœ„ì¹˜ë¥¼ ì¹˜í™˜
                    bern = (torch.rand_like(next_pred.float()) < ss_prob)
                    # í˜„ì¬ ì‹œì  í† í°ì€ "ì§ì „ ì‹œì ì˜ ëª¨ë¸ ì¶œë ¥"ì„ ë„£ëŠ” ê²Œ ë§ìŒ
                    # decoder_input[:, 1:]ì™€ next_pred[:, :-1]ë¥¼ ì •ë ¬ì‹œì¼œ ì¹˜í™˜
                    di_tail  = decoder_input[:, 1:]     # (B, T-2)
                    mix_pred = next_pred[:, :-1]        # (B, T-2)
                    decoder_input[:, 1:] = torch.where(bern[:, :-1], mix_pred, di_tail)

                with autocast():  # ìë™ í˜¼í•© ì •ë°€ë„ (FP16)      
                    outputs = model(input_ids, input_lengths, decoder_input)

                    # smoothed loss ì •ì˜ (1.5ë°°)
                    loss = label_smoothed_loss(outputs, target,
                            epsilon=0.1,
                            ignore_index=self.PAD_TOKEN_ID,
                            class_weight=class_weight)

                # Gradient Clipping
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()

                # --- í›ˆë ¨ ì¤‘ ì§„ë‹¨ ë¡œê·¸ ê³„ì‚° ---
                with torch.no_grad():
                    # 1) ìë¦¬ë§ì¶¤ ê¸°ë°˜ 'í¸ì§‘ ë¹„ìœ¨' ë¹ ë¥¸ ë¡œê·¸ (gold vs pred_tf)  â€» ë°°ì¹˜ ì „ì²´
                    tf_pred = logits_tf.argmax(dim=-1)  # (B, T-1)
                    # BOS + ì˜ˆì¸¡ìœ¼ë¡œ 'pred_full' êµ¬ì„± (ì¶œë ¥ ê¸¸ì´ë¥¼ output_idsì™€ ë§ì¶¤)
                    pred_full = torch.cat([decoder_input[:, :1], tf_pred], dim=1)  # (B, T)

                    # ìë¦¬ë§Ÿì¤Œ ê·¼ì‚¬(BOS ì œì™¸)
                    non_pad = (output_ids[:, 1:] != self.PAD_TOKEN_ID)
                    gold_edits_mask = ((output_ids[:, 1:] != input_ids[:, 1:]) & non_pad)
                    pred_edits_mask = ((pred_full[:, 1:]  != input_ids[:, 1:]) & non_pad)


                    # í† í° ë‹¨ìœ„ë¡œ ëˆ„ì (ì—í¬í¬ ì „ì²´ ê¸°ì¤€ì˜ "ë¹„ìœ¨" ê³„ì‚°ì— ì“°ì„)
                    epoch_gold_edit_tok += gold_edits_mask.sum().item()
                    epoch_pred_edit_tok += pred_edits_mask.sum().item()
                    epoch_nonpad_tok    += non_pad.sum().item()

                # 2) ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ P/R/F0.5
                if (batch_idx % LOG_ALIGN_EVERY or batch_idx == 6697) == 0:
                    with torch.no_grad():
                        S = min(8, input_ids.size(0))
                        tp = fp = fn = 0
                        for b in range(S):
                            src_seq = _strip_special(input_ids[b],  self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            tgt_seq = _strip_special(output_ids[b], self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            hyp_seq = _strip_special(pred_full[b],  self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            tpi, fpi, fni = edit_counts_via_alignment(src_seq, tgt_seq, hyp_seq)
                            tp += tpi; fp += fpi; fn += fni
                        epoch_align_tp += tp
                        epoch_align_fp += fp
                        epoch_align_fn += fn
                        epoch_align_calls += 1

                # ì˜ˆì¸¡
                pred_ids = outputs.argmax(dim=-1)
                pred_ids = pred_ids.view(output_ids.size(0), output_ids.size(1) - 1)

                # ì •í™•ë„ ê³„ì‚°
                target_2d = target.view(pred_ids.size(0), pred_ids.size(1))
                nonpad_loss = (target_2d != self.PAD_TOKEN_ID).sum().item()

                # í† í° ê°€ì¤‘ ì†ì‹¤ ëˆ„ì 
                total_loss_tok += loss.item() * nonpad_loss
                epoch_loss_tok += nonpad_loss
                
                correct_tokens += ((pred_ids == target_2d) & (target_2d != self.PAD_TOKEN_ID)).sum().item()
                total_tokens += (target_2d != self.PAD_TOKEN_ID).sum().item()

                # í¸ì§‘ ì •í™•ë„(ìë¦¬ë§ì¶¤ ë°©ì‹)
                gold_edits = ((output_ids[:, 1:] != input_ids[:, 1:]) & (output_ids[:, 1:] != self.PAD_TOKEN_ID))
                pred_edits = ((pred_ids != input_ids[:, 1:]) & (pred_ids != self.PAD_TOKEN_ID))
                correct_edits = ((pred_ids == output_ids[:, 1:]) & gold_edits)

                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()
                correct_edit_total += correct_edits.sum().item()
                # --------------------

                # ì§„í–‰ë¥  í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    #'edit_ratio': f'{edit_ratio:.2f}'
                })

                # ---- ë””ë²„ê¹…ìš© ì¶œë ¥ë¬¸ ----
                batch_nonpad = (target_2d != self.PAD_TOKEN_ID).sum().item()
                outputs_view = outputs.view(-1, outputs.size(-1))
                target_view = target.contiguous().view(-1)

                # ë¹„ì •ìƒì ì¸ outputs í…ì„œ ì²´í¬
                if not torch.isfinite(outputs_view).all():
                    print("[Debug]ë°°ì¹˜ë³„ target ë¹„-íŒ¨ë”© í† í° ìˆ˜:", batch_nonpad)
                    print("[Debug]Decoder outputs shape:", outputs_view.shape)
                    print("[Debug]Target shape:", target_view.shape)
                    print("[DebugğŸš¨] outputs í…ì„œ ë‚´ NaN/Inf ì¡´ì¬!")
                    print("ì˜ˆì‹œ ì¶œë ¥ (ì²« 5ê°œ):", outputs_view[0][:5])
                    print("ìµœëŒ€ê°’:", outputs_view.max().item(), "ìµœì†Œê°’:", outputs_view.min().item(), "í‰ê· ê°’:", outputs_view.mean().item())
                
                # ë¹„ì •ìƒì ì¸ loss ê°’ ì²´í¬
                if not torch.isfinite(loss):
                    print(f"[DebugğŸš¨] ë¹„ì •ìƒ Loss ë°œìƒ: {loss.item()}")
                    print("[Debug]í˜„ì¬ learning rate:", scheduler.get_last_lr())
                # --------------------
                
                # ìƒ˜í”Œ ì¶œë ¥ (ê° ì—í¬í¬ë§ˆë‹¤ 5ê°œ)
                if batch_idx < 5:
                    try:
                        input_text = safe_decode(tokenizer, input_ids[0].cpu().tolist())
                        output_text = safe_decode(tokenizer, output_ids[0].cpu().tolist())
                        pred_ids_with_pad = [self.BOS_TOKEN_ID] + pred_ids[0].cpu().tolist()
                        pred_text = safe_decode(tokenizer, pred_ids_with_pad)

                        # ìƒ˜í”Œ ì¶œë ¥
                        print(f"\n\tìƒ˜í”Œ {batch_idx+1}:")
                        print(f"\t> ì…ë ¥: {input_text}")
                        print(f"\t> ì˜ˆì¸¡: {pred_text}")
                        print(f"\t> ì •ë‹µ: {output_text}")

                    except Exception as e:
                        print(f"[ErrorğŸš¨] ìƒ˜í”Œ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ----- ì—í¬í¬ë³„ í‰ê·  ì†ì‹¤ ë° ì§€í‘œ ê³„ì‚° -----
            # ì†ì‹¤/í† í° ì •í™•ë„
            avg_loss = total_loss_tok / max(1, epoch_loss_tok)
            avg_edit_ratio = (epoch_pred_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0
            token_acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0

            # ìë¦¬ ë§ì¶¤ ê¸°ë°˜ P/R/F0.5
            precision = correct_edit_total / total_pred_edits if total_pred_edits > 0 else 0.0
            recall    = correct_edit_total / total_gold_edits if total_gold_edits > 0 else 0.0
            beta = 0.5
            f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0

            # ì—í¬í¬ ì „ì²´ ê¸°ì¤€ ìë¦¬ë§ì¶¤ í¸ì§‘ ë¹„ìœ¨ (í† í° ê°€ì¤‘ í‰ê· )
            epoch_gold_edit_ratio = (epoch_gold_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0
            epoch_pred_edit_ratio = (epoch_pred_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0

            # ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ P/R/F0.5
            if (epoch_align_tp + epoch_align_fp + epoch_align_fn) > 0:
                align_prec = epoch_align_tp / (epoch_align_tp + epoch_align_fp) if (epoch_align_tp + epoch_align_fp) > 0 else 0.0
                align_reca = epoch_align_tp / (epoch_align_tp + epoch_align_fn) if (epoch_align_tp + epoch_align_fn) > 0 else 0.0
                align_f05  = (1 + beta**2) * align_prec * align_reca / (beta**2 * align_prec + align_reca) if (align_prec + align_reca) > 0 else 0.0
            else:
                align_prec = align_reca = align_f05 = float('nan')

            print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}, Avg Edit Ratio(pred/token-wt): {avg_edit_ratio:.4f}")
            print(f"    Token Acc(2nd pass): {token_acc:.4f}, (pos-based) Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")
            print(f"    gold_edit_ratio={epoch_gold_edit_ratio:.3f} | pred_edit_ratio={epoch_pred_edit_ratio:.3f} | ratio={epoch_pred_edit_ratio/epoch_gold_edit_ratio if epoch_gold_edit_ratio>0 else 0:.3f}")
            print(f"    Align(P/R/F0.5)={align_prec:.3f}/{align_reca:.3f}/{align_f05:.3f} (calls={epoch_align_calls})")

            # CSV ê¸°ë¡
            csv_path = f"{transformer_path}/epoch_metrics.csv"
            write_header = (epoch == 0) and (not os.path.exists(csv_path))
            with open(csv_path, mode="a", newline="") as file:
                writer = csv.writer(file)
                if write_header:
                    writer.writerow([
                        "epoch", "loss", "edit_ratio", "token_acc", "precision", "recall", "f0.5",
                        "gold_edit_ratio_token_wt", "pred_edit_ratio_token_wt",
                        "align_prec", "align_reca", "align_f0.5", "align_calls"
                    ])
                writer.writerow([
                    epoch + 1,
                    avg_loss,
                    avg_edit_ratio,
                    token_acc,
                    precision,
                    recall,
                    f0_5,
                    epoch_gold_edit_ratio,
                    epoch_pred_edit_ratio,
                    align_prec,
                    align_reca,
                    align_f05,
                    epoch_align_calls
                ])
            # --------------------

            # í•™ìŠµë¥  ì¡°ì •
            scheduler.step(avg_loss)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if((epoch + 1) != 0):
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                    #'edit_ratio': avg_edit_ratio,
                    'tokenizer': tokenizer
                }
                torch.save(checkpoint, f"{transformer_path}/checkpoints/luna_model_epoch_{epoch+1}.pt")

    # ------------------------
    # 3. í‰ê°€ ì„¤ì •
    # ------------------------
    def evaluate(self):
        import os, torch
        from torch.utils.data import DataLoader
        from tqdm import tqdm
        import torch.nn.functional as F

        # ===== ê¸°ë³¸ ì„¤ì • =====
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        transformer_path = f"/workspace/transformer"
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path   = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')
        CHECKPOINT_DIR   = f"{transformer_path}/checkpoints"
        printed_guard = False

        # ===== í† í¬ë‚˜ì´ì €/ë°ì´í„°ì…‹ =====
        tokenizer = SentencePieceTokenizer(val_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        val_loader = DataLoader(dataset.val_dataset, batch_size=self.BATCH_SIZE, shuffle=False)

        # ===== ëª¨ë¸/criterion =====
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=self.D_MODEL,
            num_layers=self.NUM_LAYERS,
            num_attention_heads=self.NUM_HEADS,
            d_ff=self.D_FF,
            dropout_p=self.DROPOUT,
            project_embedding_length=32,
            max_length=self.MAX_SEQ_LENGTH
        ).to(device)

        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.PAD_TOKEN_ID)

        # ===== ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ =====
        ckpt_path = get_latest_checkpoint(CHECKPOINT_DIR)
        if ckpt_path is None:
            print(">> ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", CHECKPOINT_DIR)
            return
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        latest_checkpoint = os.path.basename(ckpt_path)
        epoch_num = int(latest_checkpoint.split('_')[-1].split('.')[0])
        print(f">> ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path} (epoch {epoch_num})")

        # ==== [ì¶”ê°€] free-running ìƒì„± í•¨ìˆ˜ ====
        @torch.no_grad()
        def free_run_generate(model, input_ids, input_lengths,
                            max_len, bos_id, eos_id, pad_id):
            """
            ì…ë ¥ ë°°ì¹˜ì— ëŒ€í•´ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œë¡œ ë””ì½”ë”©(teacher-forcing ì—†ìŒ).
            ë°˜í™˜: pred_full (B, T_full) : [BOS] ... EOS ... PAD
            """
            B = input_ids.size(0)
            # ì‹œì‘: [BOS] + PAD...  í˜•íƒœë¡œ ì´ˆê¸°í™”
            dec = torch.full((B, 1), bos_id, dtype=input_ids.dtype, device=input_ids.device)

            # ì´ë¯¸ EOSë¥¼ ë‚¸ ìƒ˜í”Œ ë§ˆìŠ¤í¬
            finished = torch.zeros(B, dtype=torch.bool, device=input_ids.device)

            # ë””ì½”ë”© ì œì•½ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            repetition_penalty = 1.12     # 1.1~1.2 ì¶”ì²œ
            eos_bonus = 0.25              # 0.2~0.5 ì‚¬ì´ ì‹¤í—˜
            no_repeat_ngram = 3           # 2ë‚˜ 3 ì¶”ì²œ
            max_same_token = 8            # ë™ì¼ í† í° NíšŒ ì—°ì† ë°©ì§€

            # ìµœëŒ€ ê¸¸ì´-1 ë§Œí¼ ë°˜ë³µ (BOS í¬í•¨í•˜ë¯€ë¡œ -1)
            for t in range(1, max_len):
                logits = model(input_ids, input_lengths, dec)      # (B, t, V)
                step_logits = logits[:, -1, :].clone()             # (B, V)

                # --- (1) repetition penalty ---
                # ì§€ê¸ˆê¹Œì§€ ì“´ í† í°(out=dec)ì˜ ë¹ˆë„ë¥¼ ì´ìš©í•´ ì‚¬ìš©í•œ í† í°ì˜ ë¡œì§“ì„ ì¡°ê¸ˆ ë‚®ì¶¤
                for b in range(B):
                    used = dec[b].unique()
                    step_logits[b, used] /= repetition_penalty

                # --- (2) no-repeat n-gram (ì•„ì£¼ ê°„ë‹¨ ë²„ì „) ---
                # prefixê°€ ë™ì¼í•œ n-gram ë°˜ë³µì„ ì–µì œ: ì§ì „ í† í° ì‹œí€€ìŠ¤ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ì¼ë¶€ ìƒìœ„ í›„ë³´ë¥¼ -inf ì²˜ë¦¬
                if no_repeat_ngram >= 2 and dec.size(1) >= (no_repeat_ngram - 1):
                    prefix = dec[:, -(no_repeat_ngram - 1):]    # (B, n-1)
                    # ê°„ë‹¨ ë²„ì „: ê° ë°°ì¹˜ë§ˆë‹¤ ìƒìœ„ K í›„ë³´ë¥¼ ë§‰ì•„ ë³´ìˆ˜ì ìœ¼ë¡œ ë°˜ë³µ ì–µì œ
                    K = 5
                    topk = step_logits.topk(K, dim=-1).indices  # (B, K)
                    # prefixê°€ ì§§ì„ ë• ì‹¤ì œ n-gram ì¸ë±ì‹±ì„ í•˜ì§€ ì•Šê³  ë³´ìˆ˜ì ìœ¼ë¡œ topKë¥¼ ë‚®ì¶¤
                    step_logits.scatter_(1, topk, -1e9)

                # --- (3) EOS ë³´ë„ˆìŠ¤ ---
                step_logits[:, eos_id] += eos_bonus

                # --- (4) ë™ì¼ í† í° ì—°ì† ë°©ì§€ ---
                if dec.size(1) >= max_same_token:
                    last_tok = dec[:, -1]                      # (B,)
                    run = (dec[:, -(max_same_token-1):] == last_tok.unsqueeze(1)).all(dim=1)  # Trueë©´ ì§ì „ N-1ì´ ì „ë¶€ ê°™ìŒ
                    # ì—°ì† runì¸ ë°°ì¹˜ì— ëŒ€í•´ í•´ë‹¹ í† í°ì„ ê°•ì œë¡œ ë°°ì œ
                    step_logits[run, last_tok] = -1e9

                next_ids = step_logits.argmax(dim=-1)          # (B,)
                next_ids = torch.where(finished, torch.full_like(next_ids, pad_id), next_ids)
                finished |= (next_ids == eos_id)

                dec = torch.cat([dec, next_ids.unsqueeze(1)], dim=1)
                if finished.all():
                    break

            # ê¸¸ì´ ëª¨ìë¼ë©´ PADë¡œ ìš°ì¸¡ íŒ¨ë”©
            if dec.size(1) < max_len:
                pad_cols = max_len - dec.size(1)
                pad = torch.full((B, pad_cols), pad_id, dtype=dec.dtype, device=dec.device)
                dec = torch.cat([dec, pad], dim=1)

            return dec  # (B, max_len)

        # ===== í‰ê°€ ë£¨í”„ =====
        model.eval()
        total_loss = 0.0
        total_tokens = 0
        total_eval_non_pad = 0
        correct_tokens = 0

        total_gold_edits = 0
        total_pred_edits = 0
        correct_edits = 0

        first_batch_guard_printed = False

        # ìƒ˜í”Œ ì¶œë ¥ ê°œìˆ˜ ì»¨íŠ¸ë¡¤ (ì›í•˜ë©´ 3ìœ¼ë¡œ ë°”ê¿”ë„ ë¨)
        N_SAMPLES = 5
        printed_examples = 0

        # ë””ì½”ë”©ìš© í•¨ìˆ˜: PAD ì œê±° + BOS/EOS ì˜ë¼ë‚´ê¸°
        def _strip_special(ids, bos_id=self.BOS_TOKEN_ID, eos_id=self.EOS_TOKEN_ID, pad_id=self.PAD_TOKEN_ID):
            # ë¦¬ìŠ¤íŠ¸/í…ì„œ ëª¨ë‘ ì§€ì›
            if torch.is_tensor(ids):
                ids = ids.tolist()
            # ì•ìª½ BOS ì œê±°
            if ids and ids[0] == bos_id:
                ids = ids[1:]
            # EOS ì´í›„ ì˜ë¼ë‚´ê¸°
            if eos_id in ids:
                cut = ids.index(eos_id)
                ids = ids[:cut]
            # PAD ì œê±°
            ids = [t for t in ids if t != pad_id]
            return ids

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids  = batch['input_ids'].to(device)     # (B, T_full)
                output_ids = batch['output_ids'].to(device)    # (B, T_full) = [BOS ... EOS PAD...]

                input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1)

                # ---- teacher-forcing ê¸°ë°˜ lossë§Œ ê³„ì‚° (ì§„ë‹¨ìš©) ----
                decoder_input = output_ids[:, :-1].contiguous()
                target        = output_ids[:,  1:].contiguous()   # (B, T-1)

                logits = model(input_ids, input_lengths, decoder_input)  # (B, T-1, V)

                if not first_batch_guard_printed:
                    # ì •ë ¬/shape ê°€ë“œ
                    eos_pos = (output_ids[0] != self.PAD_TOKEN_ID).sum() - 1
                    print("[VAL] BOS/EOS (should be 2,3):", output_ids[0,0].item(), output_ids[0, eos_pos].item())
                    print("[VAL] shapes (logits vs target):", logits.shape, target.shape)
                    print("[VAL] non-pad ratio in target:", (target != self.PAD_TOKEN_ID).float().mean().item())
                    print("[VAL] sample target head:", target[0, :10].tolist())
                    print("[VAL] sample pred head  :", logits.argmax(-1)[0, :10].tolist())
                    print("[VAL] sample decoder_inp:", decoder_input[0, :10].tolist())
                    first_batch_guard_printed = True

                loss = criterion(logits.view(-1, self.VOCAB_SIZE), target.view(-1))
                non_pad_mask = (target != self.PAD_TOKEN_ID)
                num_non_pad = non_pad_mask.sum().item()
                total_loss += loss.item() * num_non_pad
                total_tokens += num_non_pad

                # ---- (Eâ€™) free-running ìƒì„±ìœ¼ë¡œ ì˜ˆì¸¡/ì§€í‘œ ê³„ì‚° ----
                pred_full = free_run_generate(
                    model=model,
                    input_ids=input_ids,
                    input_lengths=input_lengths,
                    max_len=output_ids.size(1),   # ì •ë‹µ ê¸¸ì´ì— ë§ì¶° ìƒì„±
                    bos_id=self.BOS_TOKEN_ID,
                    eos_id=self.EOS_TOKEN_ID,
                    pad_id=self.PAD_TOKEN_ID
                )  # (B, T_full)

                # í† í° ì •í™•ë„ëŠ” free-running ì˜ˆì¸¡ ê¸°ì¤€ìœ¼ë¡œ(ì‹¤ì‚¬ìš©ê³¼ ì¼ì¹˜)
                non_pad_full = (output_ids != self.PAD_TOKEN_ID)
                correct_tokens += ((pred_full == output_ids) & non_pad_full).sum().item()

                total_eval_non_pad += non_pad_full.sum().item()

                # í¸ì§‘ ì§€í‘œ(ì…ë ¥/ì •ë‹µ/ì˜ˆì¸¡ì„ ê°™ì€ í”„ë ˆì„ì—ì„œ ë¹„êµ)
                valid_mask = (output_ids != self.PAD_TOKEN_ID) & (output_ids != self.BOS_TOKEN_ID) & (output_ids != self.EOS_TOKEN_ID)
                gold_edits = (output_ids != input_ids) & valid_mask
                pred_edits = (pred_full  != input_ids) & valid_mask

                correct_edits   += ((pred_full == output_ids) & gold_edits).sum().item()
                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()

                # ë°°ì¹˜ì—ì„œ ëª‡ ê°œ ë½‘ì•„ "ì…ë ¥/ì˜ˆì¸¡/ì •ë‹µ + ì²« í† í° ë¹„êµ" ì¶œë ¥
                if printed_examples < N_SAMPLES:
                    bsz = input_ids.size(0)
                    # ê°€ëŠ¥í•œ ë§Œí¼ë§Œ ì¶œë ¥
                    take = min(N_SAMPLES - printed_examples, bsz)
                    for i in range(take):
                        # ì²« í† í° ë¹„êµ(ì˜ˆì¸¡ì€ pred_full[:,1], ì •ë‹µì€ output_ids[:,1])
                        try:
                            pred_first_id = pred_full[i, 1].item()
                            gold_first_id = output_ids[i, 1].item()
                            pred_first = safe_decode(tokenizer, [pred_first_id])
                            gold_first = safe_decode(tokenizer, [gold_first_id])
                            print(f"> ì²« í† í° ë¹„êµ | ì˜ˆì¸¡: {pred_first} / ì •ë‹µ: {gold_first}")
                        except Exception as e:
                            print(f"> ì²« í† í° ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")

                        # ë³¸ë¬¸ ë””ì½”ë”©(íŠ¹ìˆ˜í† í°/íŒ¨ë”© ì œê±°)
                        in_ids   = _strip_special(input_ids[i])
                        pr_ids   = _strip_special(pred_full[i])
                        out_ids  = _strip_special(output_ids[i])

                        input_text  = safe_decode(tokenizer, in_ids)
                        pred_text   = safe_decode(tokenizer, pr_ids)
                        output_text = safe_decode(tokenizer, out_ids)

                        print(f"\n\tìƒ˜í”Œ {printed_examples + 1}:")
                        print(f"\t> ì…ë ¥: {input_text}")
                        print(f"\t> ì˜ˆì¸¡: {pred_text}")
                        print(f"\t> ì •ë‹µ: {output_text}")

                        printed_examples += 1
                        if printed_examples >= N_SAMPLES:
                            break

                if not printed_guard:
                    eos_rate = (pred_full[:, 1:] == self.EOS_TOKEN_ID).float().mean().item()
                    print("[VAL] EOS rate(after first token):", eos_rate)

                    # ì…ë ¥ ë¬´ì‹œ ì—¬ë¶€ë¥¼ ê°„ë‹¨íˆ ì²´í¬(ìƒ˜í”Œ ëª‡ ê°œë§Œ ì„ì–´ì„œ)
                    idx = torch.arange(input_ids.size(0), device=input_ids.device)
                    idx = idx[torch.randperm(idx.numel())[: min(8, idx.numel())]]
                    pred_shuf = free_run_generate(
                        model, input_ids[idx], (input_ids[idx] != self.PAD_TOKEN_ID).sum(1),
                        max_len=output_ids.size(1),
                        bos_id=self.BOS_TOKEN_ID, eos_id=self.EOS_TOKEN_ID, pad_id=self.PAD_TOKEN_ID
                    )
                    # EOS rateê°€ 0ì— ê°€ê¹ë‹¤ë©´ EOSê°€ ê±°ì˜ ì•ˆ ë‚˜ì˜´ â†’ ë””ì½”ë”© ë¬´í•œë°˜ë³µ ê²½í–¥
                    same_ratio = (pred_full[idx] == pred_shuf).float().mean().item()
                    print("[VAL] Input-agnostic ratio:", same_ratio)

                    def _strip(ids, BOS, EOS, PAD):
                        if torch.is_tensor(ids): ids = ids.tolist()
                        if ids and ids[0] == BOS: ids = ids[1:]
                        if EOS in ids:
                            ids = ids[:ids.index(EOS)]
                        return [t for t in ids if t != PAD]

                    # ë°°ì¹˜ì—ì„œ 4ê°œë§Œ ìƒ˜í”Œ ë¹„êµ
                    k = min(4, pred_full.size(0))
                    same = 0
                    for j in range(k):
                        a = _strip(pred_full[j], self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                        b = _strip(pred_shuf[j], self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                        same += 1.0 if a == b else 0.0
                    print("[VAL] Input-agnostic (stripped) mean:", same / k)

                    printed_guard = True

        # ===== ì§€í‘œ ì§‘ê³„ =====
        if total_tokens > 0:
            avg_loss  = total_loss / total_tokens        # teacher-forcing loss (ì§„ë‹¨ìš©)
        else:
            avg_loss = float('nan')

        # free-running ê¸°ì¤€ í† í° ì •í™•ë„/í¸ì§‘ ì§€í‘œ
        token_acc = correct_tokens / total_eval_non_pad if total_eval_non_pad > 0 else 0.0
        precision = (correct_edits / total_pred_edits) if total_pred_edits > 0 else 0.0
        recall    = (correct_edits / total_gold_edits) if total_gold_edits > 0 else 0.0
        beta = 0.5
        f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0

        print(f"\n>> Validation ê²°ê³¼ (epoch {epoch_num}) [free-running]:")
        print(f"   (TF-loss ì§„ë‹¨) í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        print(f"   Token Acc: {token_acc:.4f}")
        print(f"   Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")

if __name__ == '__main__':
    tne = pNup_s2s()
    tne.train()
    tne.evaluate()