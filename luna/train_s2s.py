# Luna ì‚¬ì „í•™ìŠµ + ë¯¸ì„¸ì¡°ì • (Gradient Checkpointing + FP16 + DeepSpeed ZeRO)

import os, sys, csv
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
import numpy as np
from tqdm import tqdm

drive_path = "/workspace"
# Colab í™˜ê²½ì—ì„œëŠ” drive_pathë¡œ ì´ë™ì´ í•„ìš” -> ì•„ë˜ì˜ í´ë˜ìŠ¤í‹€ì„ import í•˜ê¸° ìœ„í•¨
#%cd "$drive_path"

from transformer.luna.model import LunaTransformerEncoder, LunaTransformer
from transformer.code_transformer.dataset import SpellingDataset
from transformer.code_transformer.sentencepiece_tokenizer import SentencePieceTokenizer

# ------------------------
# ì•ˆì „í•œ ë””ì½”ë”© í•¨ìˆ˜
# ------------------------
def safe_decode(tokenizer, ids):
    try:
        # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ID í•„í„°ë§
        vocab_size = tokenizer.get_piece_size()
        valid_ids = [id for id in ids if id < vocab_size]

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if not valid_ids:
            return "<ë¹ˆ ì‹œí€€ìŠ¤>"

        return tokenizer.decode(valid_ids)
    except Exception as e:
        return f"ë””ì½”ë”© ì˜¤ë¥˜: {e}"

# ------------------------
# ê°€ì¥ ìµœì‹ ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ path ë°˜í™˜
# ------------------------
"""
    train, evaluate í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–‘ì‹ì„ ê·¸ëŒ€ë¡œ í•¨ìˆ˜ë¡œ ì˜®ê²¨ì„œ ì •ì˜
"""
def get_latest_checkpoint(checkpoint_dir):
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
    if not checkpoints:
        raise FileNotFoundError("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
    return os.path.join(checkpoint_dir, latest_checkpoint)

# ------------------------
# 1. Luna ëª¨ë¸ ì •ì˜
# ------------------------
"""
    ì œê±° ë˜ëŠ” model.pyë¡œ ì´ë™
"""
class LunaModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_attention_heads=8):
        super().__init__()
        self.encoder = LunaTransformerEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            num_layers=num_layers,
            num_attention_heads=num_attention_heads,
            d_ff=2048,
            dropout_p=0.2,
            project_embedding_length=32,
            max_length=1024
        )
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, input_lengths):
        # ì¸ì½”ë”ë¥¼ í†µê³¼
        encoder_outputs = self.encoder(input_ids, input_lengths)
        # ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ í†µê³¼
        logits = self.lm_head(encoder_outputs)
        return logits

# ------------------------
# 2. í•™ìŠµ ì„¤ì •
# ------------------------
"""
    ì£¼ì„ ì¶”ê°€ í•„ìš”
"""
def label_smoothed_loss(pred, target, epsilon=0.1, ignore_index=0, class_weight=None):
    """
    pred: (B*T, V)  - ë¡œì§“
    target: (B*T,)  - ì •ë‹µ í† í° id
    epsilon: ë¼ë²¨ ìŠ¤ë¬´ë”© ê³„ìˆ˜
    ignore_index: PAD id (ì˜ˆ: 0)
    class_weight: (V,) or None  - EOS ë“±ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ì‹œ ì‚¬ìš©
    """
    V = pred.size(-1)
    log_probs = F.log_softmax(pred, dim=-1)          # (N, V)

    # ìœ íš¨ í† í° ë§ˆìŠ¤í¬
    mask = (target != ignore_index)                  # (N,)
    if mask.sum() == 0:
        return pred.new_tensor(0.0)

    target_clamped = target.clone()
    target_clamped[~mask] = 0                        # one_hotì˜ ì¸ë±ìŠ¤ ì•ˆì „í™”

    one_hot = F.one_hot(target_clamped, num_classes=V).float()  # (N, V)
    # ë¼ë²¨ ìŠ¤ë¬´ë”©
    smoothed = (1 - epsilon) * one_hot + (epsilon / V)

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì˜ˆ: EOS 1.5)
    if class_weight is not None:
        # (V,) -> (N,V) ë¸Œë¡œë“œìºìŠ¤íŠ¸
        smoothed = smoothed * class_weight.unsqueeze(0)

    # í† í°ë³„ ì†ì‹¤
    loss_per_tok = -(smoothed * log_probs).sum(dim=-1)          # (N,)
    loss = loss_per_tok[mask].mean()
    return loss

# --- ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ìš© ---
import difflib

def _strip_special(ids, BOS, EOS, PAD):
    if torch.is_tensor(ids):
        ids = ids.tolist()
    if ids and ids[0] == BOS:
        ids = ids[1:]
    if EOS in ids:
        ids = ids[:ids.index(EOS)]
    return [t for t in ids if t != PAD]

def edit_counts_via_alignment(src_tokens, tgt_tokens, hyp_tokens):
    """
    srcâ†’tgt, srcâ†’hyp ë³€í™˜ì—ì„œ ë¹„-ë™ì¼ êµ¬ê°„ì„ í¸ì§‘ìœ¼ë¡œ ë³´ê³ ,
    src êµ¬ê°„ ê²¹ì¹¨ìœ¼ë¡œ TPë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ ì…‰ë‹ˆë‹¤. (ë¹ ë¥¸ ëª¨ë‹ˆí„°ë§ìš©)
    """
    sm_gold = difflib.SequenceMatcher(a=src_tokens, b=tgt_tokens)
    gold_spans = [op for op in sm_gold.get_opcodes() if op[0] != 'equal']

    sm_pred = difflib.SequenceMatcher(a=src_tokens, b=hyp_tokens)
    pred_spans = [op for op in sm_pred.get_opcodes() if op[0] != 'equal']

    tp = 0
    for tag, gi1, gi2, gj1, gj2 in gold_spans:
        for tag2, pi1, pi2, pj1, pj2 in pred_spans:
            # src ê¸°ì¤€ êµ¬ê°„ì´ ê²¹ì¹˜ë©´ TPë¡œ ì¹´ìš´íŠ¸
            if not (pi2 <= gi1 or pi1 >= gi2):
                tp += 1
                break

    fp = max(0, len(pred_spans) - tp)
    fn = max(0, len(gold_spans) - tp)
    return tp, fp, fn

"""
    train, evaluate í•¨ìˆ˜ëŠ” í•˜ë‚˜ë¡œ ë‘ê³ , íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ ì œì–´í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •.
        - train, evaluateì— íŒŒë¼ë¯¸í„° model_type ì¶”ê°€
        - model_typeì— ëŒ€í•œ ì‚¬ì „ê°’ì€ __init__ì—ì„œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¡œ ì •ì˜
"""
class pNup_s2s:
    def __init__(self):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        self.BATCH_SIZE = 32
        self.EPOCHS = 50
        self.LEARNING_RATE = 0.0001
        self.D_MODEL = 512
        self.NUM_HEADS = 8
        self.NUM_LAYERS = 6
        self.D_FF = 2048
        self.MAX_SEQ_LENGTH = 128
        self.DROPOUT = 0.2
        self.VOCAB_SIZE = 16000
        self.PAD_TOKEN_ID = 0    # íŒ¨ë”© í† í° ID
        self.BOS_TOKEN_ID = 2    # ì‹œì‘ í† í° ID( <s> or [BOS] )
        self.EOS_TOKEN_ID = 3    # ì¢…ë£Œ í† í° ID

    def train(self):
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {device}")

        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •
        """ 
            í•´ë‹¹ ë³€ìˆ˜ë“¤ì€ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë³€ê²½ì´ ìˆì§€ ì•ŠëŠ” í•œ ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë  ê²ƒì„. 
                -> __init__ì—ì„œ ì •ì˜ ë˜ëŠ” ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜
        """
        transformer_path = "/workspace/transformer"
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')
        tokenizer = SentencePieceTokenizer(train_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        train_loader = DataLoader(dataset.train_dataset, batch_size=self.BATCH_SIZE, shuffle=True)

        checkpoints = [f for f in os.listdir(f"{transformer_path}/checkpoints") if f.endswith('.pt')]
        
        # ëª¨ë¸ ê°ì²´ ìƒì„±
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=self.D_MODEL,
            num_layers=self.NUM_LAYERS,
            num_attention_heads=self.NUM_HEADS,
            d_ff = self.D_FF,
            dropout_p = self.DROPOUT,
            project_embedding_length = 32,
            max_length = self.MAX_SEQ_LENGTH
        )
        model = model.to(device)

        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.LEARNING_RATE, weight_decay=0.01)
        scaler = GradScaler(device_type)  # FP16ì„ ìœ„í•œ Gradient Scaler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=2
        )

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        """ $ì¶”ê°€. íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ ì¶”ê°€í•  ê²ƒ """
        if not checkpoints:
            print("ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ.")
            latest_checknum = 0
        else:
            # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint_path = get_latest_checkpoint(f"{transformer_path}/checkpoints")
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

            # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
            latest_checkpoint = os.path.basename(checkpoint_path)
            latest_checknum = int(latest_checkpoint.split('_')[-1].split('.')[0])

            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            model = model.to(device)

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (EOS í† í°ì— 1.5ë°° ê°€ì¤‘ì¹˜ ë¶€ì—¬)
        eos_weight = 1.5
        class_weight = torch.ones(self.VOCAB_SIZE, device=device)
        class_weight[self.EOS_TOKEN_ID] = eos_weight

        # í•™ìŠµ ë£¨í”„
        model.train()
        for epoch in range(latest_checknum, self.EPOCHS + latest_checknum):        
            epoch_gold_edit_tok = 0
            epoch_pred_edit_tok = 0
            epoch_nonpad_tok    = 0

            epoch_align_tp = 0
            epoch_align_fp = 0
            epoch_align_fn = 0
            epoch_align_calls = 0

            total_gold_edits = 0
            total_pred_edits = 0

            correct_edit_total = 0
            correct_tokens = 0
            total_tokens = 0

            total_loss_tok = 0
            epoch_loss_tok = 0

            LOG_ALIGN_EVERY = 500
            
            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.EPOCHS+latest_checknum}')

            for batch_idx, batch in enumerate(progress_bar):
                """
                    1. Beam Search ì¶”ê°€.
                    2. ê¸¸ì´ ì •ê·œí™” ë° íŒ¨ë„í‹° ì¶”ê°€.
                    3. ë°˜ë³µ íŒ¨ë„í‹° ìˆ˜ì •(ì´ˆì•ˆì—ì„œëŠ” ì ìš©X. í•„ìš”ì‹œ ì¶”ê°€)
                    4. ìŠ¤ì¼€ì¤„ë“œ ìƒ˜í”Œë§ ìˆ˜ì • ë° ë‚´ìš© ì •ë¦¬(3ê³¼ ì—°ê³„)
                    5. í‰ê°€ ë°©ì‹ ë³€ê²½(ìœ„ì¹˜ë³„ ì •í™•ë„ â†’ í¸ì§‘ ì •í™•ë„ : ì •ë ¬ê¸°ë°˜(Levenshtein/ERRANT)ìœ¼ë¡œ í¸ì§‘ì„ ë¹„êµí•˜ì—¬ P/R/F0.5 ì‚°ì¶œ)
                    6. ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒì„ ìœ„í•´ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ì ìš©.
                        - ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜í™”
                        - í•´ë‹¹ ë¶€ë¶„ì€ ìˆ˜ì • ë¼ì¸ ë°–ì˜ ë¶€ë¶„ë„ ë™ì¼í•˜ê²Œ ì ìš©
                    7. ë””ë²„ê¹…ìš© ì¶œë ¥ë¬¸ ì •ë¦¬
                        - ì¤‘ê°„ë‹¨ê³„ì—ì„œì˜ ì¶œë ¥ì´ í•„ìˆ˜ì ì´ì§€ ì•Šìœ¼ë©´ ì œê±° ë˜ëŠ” í›„ë°©ìœ¼ë¡œ ì´ë™
                    8. ì£¼ì„ ì •ë¦¬
                        - í•¨ìˆ˜í™” í•  ê²½ìš°, íŒŒë¼ë¯¸í„°ë¥¼ ëª…í™•íˆ ì£¼ì„ìœ¼ë¡œ í‘œì‹œ
                """
                # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
                input_ids   = batch['input_ids'].to(device)
                output_ids  = batch['output_ids'].to(device)
                
                # input_lengths ê³„ì‚° (íŒ¨ë”© í† í° 0ì„ ì œì™¸í•œ ì‹¤ì œ ê¸¸ì´)
                input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1).to(device)

                decoder_input   = output_ids[:, :-1]    # Decoder ì…ë ¥
                target          = output_ids[:, 1:]     # ì •ë‹µ
                optimizer.zero_grad()

                # ---- 1ì°¨ forward: teacher-forcingìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜ì§‘ ----
                with torch.no_grad():
                    logits_tf = model(input_ids, input_lengths, decoder_input)  # (B, T-1, V)
                    next_pred = logits_tf.argmax(dim=-1)                        # (B, T-1)

                # ---- ìŠ¤ì¼€ì¤„ë“œ ìƒ˜í”Œë§ í™•ë¥ : ì—í¬í¬ì— ë”°ë¼ ì ì§„ ì¦ê°€ ----
                max_ss = 0.25  # ìµœëŒ“ê°’(0.1~0.25 ê¶Œì¥)
                # latest_checknumì´ ìˆëŠ” ì½”ë“œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•´ ì§„í–‰ë¥  ê³„ì‚°
                ss_prob = min(max_ss, ((epoch - latest_checknum + 1) / self.EPOCHS) * max_ss)

                # ---- ì¼ë¶€ í† í°ì„ ëª¨ë¸ ì˜ˆì¸¡ìœ¼ë¡œ ì¹˜í™˜(ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ì¼ê´€ì„±) ----
                if ss_prob > 0.0:
                    # bern: (B, T-1)ì—ì„œ Trueì¸ ìœ„ì¹˜ë¥¼ ì¹˜í™˜
                    bern = (torch.rand_like(next_pred.float()) < ss_prob)
                    # í˜„ì¬ ì‹œì  í† í°ì€ "ì§ì „ ì‹œì ì˜ ëª¨ë¸ ì¶œë ¥"ì„ ë„£ëŠ” ê²Œ ë§ìŒ
                    # decoder_input[:, 1:]ì™€ next_pred[:, :-1]ë¥¼ ì •ë ¬ì‹œì¼œ ì¹˜í™˜
                    di_tail  = decoder_input[:, 1:]     # (B, T-2)
                    mix_pred = next_pred[:, :-1]        # (B, T-2)
                    decoder_input[:, 1:] = torch.where(bern[:, :-1], mix_pred, di_tail)

                with autocast(device_type=device_type, dtype=torch.float16):  # ìë™ í˜¼í•© ì •ë°€ë„ (FP16)      
                    outputs = model(input_ids, input_lengths, decoder_input)

                    # smoothed loss ì •ì˜ (1.5ë°°)
                    loss = label_smoothed_loss(outputs, target,
                            epsilon=0.1,
                            ignore_index=self.PAD_TOKEN_ID,
                            class_weight=class_weight)

                # Gradient Clipping
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()

                # --- í›ˆë ¨ ì¤‘ ì§„ë‹¨ ë¡œê·¸ ê³„ì‚° ---
                with torch.no_grad():
                    # 1) ìë¦¬ë§ì¶¤ ê¸°ë°˜ 'í¸ì§‘ ë¹„ìœ¨' ë¹ ë¥¸ ë¡œê·¸ (gold vs pred_tf)  â€» ë°°ì¹˜ ì „ì²´
                    tf_pred = logits_tf.argmax(dim=-1)  # (B, T-1)
                    # BOS + ì˜ˆì¸¡ìœ¼ë¡œ 'pred_full' êµ¬ì„± (ì¶œë ¥ ê¸¸ì´ë¥¼ output_idsì™€ ë§ì¶¤)
                    pred_full = torch.cat([decoder_input[:, :1], tf_pred], dim=1)  # (B, T)

                    # ìë¦¬ë§Ÿì¤Œ ê·¼ì‚¬(BOS ì œì™¸)
                    non_pad = (output_ids[:, 1:] != self.PAD_TOKEN_ID)
                    gold_edits_mask = ((output_ids[:, 1:] != input_ids[:, 1:]) & non_pad)
                    pred_edits_mask = ((pred_full[:, 1:]  != input_ids[:, 1:]) & non_pad)


                    # í† í° ë‹¨ìœ„ë¡œ ëˆ„ì (ì—í¬í¬ ì „ì²´ ê¸°ì¤€ì˜ "ë¹„ìœ¨" ê³„ì‚°ì— ì“°ì„)
                    epoch_gold_edit_tok += gold_edits_mask.sum().item()
                    epoch_pred_edit_tok += pred_edits_mask.sum().item()
                    epoch_nonpad_tok    += non_pad.sum().item()

                # 2) ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ P/R/F0.5
                if (batch_idx % LOG_ALIGN_EVERY or batch_idx == 6697) == 0:
                    with torch.no_grad():
                        S = min(8, input_ids.size(0))
                        tp = fp = fn = 0
                        for b in range(S):
                            src_seq = _strip_special(input_ids[b],  self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            tgt_seq = _strip_special(output_ids[b], self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            hyp_seq = _strip_special(pred_full[b],  self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                            tpi, fpi, fni = edit_counts_via_alignment(src_seq, tgt_seq, hyp_seq)
                            tp += tpi; fp += fpi; fn += fni
                        epoch_align_tp += tp
                        epoch_align_fp += fp
                        epoch_align_fn += fn
                        epoch_align_calls += 1

                # ì˜ˆì¸¡
                pred_ids = outputs.argmax(dim=-1)
                pred_ids = pred_ids.view(output_ids.size(0), output_ids.size(1) - 1)

                # ì •í™•ë„ ê³„ì‚°
                target_2d = target.view(pred_ids.size(0), pred_ids.size(1))
                nonpad_loss = (target_2d != self.PAD_TOKEN_ID).sum().item()

                # í† í° ê°€ì¤‘ ì†ì‹¤ ëˆ„ì 
                total_loss_tok += loss.item() * nonpad_loss
                epoch_loss_tok += nonpad_loss
                
                correct_tokens += ((pred_ids == target_2d) & (target_2d != self.PAD_TOKEN_ID)).sum().item()
                total_tokens += (target_2d != self.PAD_TOKEN_ID).sum().item()

                # í¸ì§‘ ì •í™•ë„(ìë¦¬ë§ì¶¤ ë°©ì‹)
                gold_edits = ((output_ids[:, 1:] != input_ids[:, 1:]) & (output_ids[:, 1:] != self.PAD_TOKEN_ID))
                pred_edits = ((pred_ids != input_ids[:, 1:]) & (pred_ids != self.PAD_TOKEN_ID))
                correct_edits = ((pred_ids == output_ids[:, 1:]) & gold_edits)

                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()
                correct_edit_total += correct_edits.sum().item()
                # --------------------

                # ì§„í–‰ë¥  í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    #'edit_ratio': f'{edit_ratio:.2f}'
                })

                # ---- ë””ë²„ê¹…ìš© ì¶œë ¥ë¬¸ ----
                batch_nonpad = (target_2d != self.PAD_TOKEN_ID).sum().item()
                outputs_view = outputs.view(-1, outputs.size(-1))
                target_view = target.contiguous().view(-1)

                # ë¹„ì •ìƒì ì¸ outputs í…ì„œ ì²´í¬
                if not torch.isfinite(outputs_view).all():
                    print("[Debug]ë°°ì¹˜ë³„ target ë¹„-íŒ¨ë”© í† í° ìˆ˜:", batch_nonpad)
                    print("[Debug]Decoder outputs shape:", outputs_view.shape)
                    print("[Debug]Target shape:", target_view.shape)
                    print("[DebugğŸš¨] outputs í…ì„œ ë‚´ NaN/Inf ì¡´ì¬!")
                    print("ì˜ˆì‹œ ì¶œë ¥ (ì²« 5ê°œ):", outputs_view[0][:5])
                    print("ìµœëŒ€ê°’:", outputs_view.max().item(), "ìµœì†Œê°’:", outputs_view.min().item(), "í‰ê· ê°’:", outputs_view.mean().item())
                
                # ë¹„ì •ìƒì ì¸ loss ê°’ ì²´í¬
                if not torch.isfinite(loss):
                    print(f"[DebugğŸš¨] ë¹„ì •ìƒ Loss ë°œìƒ: {loss.item()}")
                    print("[Debug]í˜„ì¬ learning rate:", scheduler.get_last_lr())
                # --------------------
                
                # ìƒ˜í”Œ ì¶œë ¥ (ê° ì—í¬í¬ë§ˆë‹¤ 5ê°œ)
                if batch_idx < 5:
                    try:
                        input_text = safe_decode(tokenizer, input_ids[0].cpu().tolist())
                        output_text = safe_decode(tokenizer, output_ids[0].cpu().tolist())
                        pred_ids_with_pad = [self.BOS_TOKEN_ID] + pred_ids[0].cpu().tolist()
                        pred_text = safe_decode(tokenizer, pred_ids_with_pad)

                        # ìƒ˜í”Œ ì¶œë ¥
                        print(f"\n\tìƒ˜í”Œ {batch_idx+1}:")
                        print(f"\t> ì…ë ¥: {input_text}")
                        print(f"\t> ì˜ˆì¸¡: {pred_text}")
                        print(f"\t> ì •ë‹µ: {output_text}")

                    except Exception as e:
                        print(f"[ErrorğŸš¨] ìƒ˜í”Œ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ----- ì—í¬í¬ë³„ í‰ê·  ì†ì‹¤ ë° ì§€í‘œ ê³„ì‚° -----
            # ì†ì‹¤/í† í° ì •í™•ë„
            avg_loss = total_loss_tok / max(1, epoch_loss_tok)
            avg_edit_ratio = (epoch_pred_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0
            token_acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0

            # ìë¦¬ ë§ì¶¤ ê¸°ë°˜ P/R/F0.5
            precision = correct_edit_total / total_pred_edits if total_pred_edits > 0 else 0.0
            recall    = correct_edit_total / total_gold_edits if total_gold_edits > 0 else 0.0
            beta = 0.5
            f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0

            # ì—í¬í¬ ì „ì²´ ê¸°ì¤€ ìë¦¬ë§ì¶¤ í¸ì§‘ ë¹„ìœ¨ (í† í° ê°€ì¤‘ í‰ê· )
            epoch_gold_edit_ratio = (epoch_gold_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0
            epoch_pred_edit_ratio = (epoch_pred_edit_tok / epoch_nonpad_tok) if epoch_nonpad_tok > 0 else 0.0

            # ì •ë ¬ ê¸°ë°˜ ê·¼ì‚¬ P/R/F0.5
            if (epoch_align_tp + epoch_align_fp + epoch_align_fn) > 0:
                align_prec = epoch_align_tp / (epoch_align_tp + epoch_align_fp) if (epoch_align_tp + epoch_align_fp) > 0 else 0.0
                align_reca = epoch_align_tp / (epoch_align_tp + epoch_align_fn) if (epoch_align_tp + epoch_align_fn) > 0 else 0.0
                align_f05  = (1 + beta**2) * align_prec * align_reca / (beta**2 * align_prec + align_reca) if (align_prec + align_reca) > 0 else 0.0
            else:
                align_prec = align_reca = align_f05 = float('nan')

            print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}, Avg Edit Ratio(pred/token-wt): {avg_edit_ratio:.4f}")
            print(f"    Token Acc(2nd pass): {token_acc:.4f}, (pos-based) Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")
            print(f"    gold_edit_ratio={epoch_gold_edit_ratio:.3f} | pred_edit_ratio={epoch_pred_edit_ratio:.3f} | ratio={epoch_pred_edit_ratio/epoch_gold_edit_ratio if epoch_gold_edit_ratio>0 else 0:.3f}")
            print(f"    Align(P/R/F0.5)={align_prec:.3f}/{align_reca:.3f}/{align_f05:.3f} (calls={epoch_align_calls})")

            # CSV ê¸°ë¡
            csv_path = f"{transformer_path}/epoch_metrics.csv"
            write_header = (epoch == 0) and (not os.path.exists(csv_path))
            with open(csv_path, mode="a", newline="") as file:
                writer = csv.writer(file)
                if write_header:
                    writer.writerow([
                        "epoch", "loss", "edit_ratio", "token_acc", "precision", "recall", "f0.5",
                        "gold_edit_ratio_token_wt", "pred_edit_ratio_token_wt",
                        "align_prec", "align_reca", "align_f0.5", "align_calls"
                    ])
                writer.writerow([
                    epoch + 1,
                    avg_loss,
                    avg_edit_ratio,
                    token_acc,
                    precision,
                    recall,
                    f0_5,
                    epoch_gold_edit_ratio,
                    epoch_pred_edit_ratio,
                    align_prec,
                    align_reca,
                    align_f05,
                    epoch_align_calls
                ])
            # --------------------

            # í•™ìŠµë¥  ì¡°ì •
            scheduler.step(avg_loss)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if((epoch + 1) != 0):
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                    #'edit_ratio': avg_edit_ratio,
                    'tokenizer': tokenizer
                }
                torch.save(checkpoint, f"{transformer_path}/checkpoints/luna_model_epoch_{epoch+1}.pt")

    # ------------------------
    # 3. í‰ê°€ ì„¤ì •
    # ------------------------
    def evaluate(self,
        beam_size: int = 4,
        max_gen_len: int = 127,         # ë””ì½”ë” í† í° ìˆ˜(BOS ì œì™¸) ìƒí•œ (MAX_LENGTH-1 ê¶Œì¥)
        length_alpha: float = 0.6,      # ê¸¸ì´ íŒ¨ë„í‹° alpha (0.6~1.0 íŠœë‹)
        repetition_penalty: float = 1.1,# 1.0ì´ë©´ ë¹„í™œì„±
        no_repeat_ngram_size: int = 3,  # 0ì´ë©´ ë¹„í™œì„±(ê¶Œì¥: 2~4)
        diag_tf: bool = False,          # Trueë©´ TF-loss/TF-accë„ ë³‘í–‰ ê³„ì‚°(ëŠë ¤ì§)
        dump_for_errant: bool = True,   # ERRANT ì…ë ¥ ë¤í”„ ì €ì¥
        dump_dir_name: str = "eval_dumps"
    ):
        """
        1) BeamSearch(+length penalty, repetition penalty, no-repeat n-gram)ë¡œ í”„ë¦¬ëŸ¬ë‹ ìƒì„±
        2) ì •ë ¬ ê¸°ë°˜ í¸ì§‘ì§€í‘œ(ê·¼ì‚¬)ë¡œ Precision/Recall/F0.5 ê³„ì‚°
        3) (ì˜µì…˜) TF-loss/TF-Acc ì§„ë‹¨
        4) (ì˜µì…˜) ERRANT ë¤í”„(src/hyp/ref) ì €ì¥
        5) ìƒ˜í”Œ 5ê°œ: ì…ë ¥/ì˜ˆì¸¡/ì •ë‹µ í˜•ì‹ ì¶œë ¥ (trainê³¼ ë™ì¼ ìŠ¤íƒ€ì¼)
        """
        import os, math, difflib, csv
        from tqdm import tqdm
        import torch
        import torch.nn.functional as F
        from torch.utils.data import DataLoader

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        # ----- í™˜ê²½/ê²½ë¡œ(ê¸°ì¡´ train ì„¤ì •ê³¼ ì¼ì¹˜) -----
        CHECKPOINT_DIR   = f"{drive_path}/transformer/checkpoints"
        transformer_path = f"{drive_path}/transformer"

        # ----- ë°ì´í„°/í† í¬ë‚˜ì´ì €/ë¡œë” -----
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path   = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')

        tokenizer = SentencePieceTokenizer(train_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset   = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        val_loader = DataLoader(dataset.val_dataset, batch_size=self.BATCH_SIZE, shuffle=False)

        # ----- ëª¨ë¸ -----
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=512,
            num_layers=6,
            num_attention_heads=8,
            d_ff=2048,
            dropout_p=0.1,
            project_embedding_length=32,
            max_length=self.MAX_SEQ_LENGTH
        ).to(device)

        # (ì˜µì…˜) TF ì§„ë‹¨ì—ì„œë§Œ ì‚¬ìš©
        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.PAD_TOKEN_ID)

        # ----- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ -----
        ckpt_path = get_latest_checkpoint(CHECKPOINT_DIR)
        if ckpt_path is None:
            print(">> ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", CHECKPOINT_DIR)
            return
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        latest_checkpoint = os.path.basename(ckpt_path)
        try:
            epoch_num = int(latest_checkpoint.split('_')[-1].split('.')[0])
        except:
            epoch_num = -1
        print(f">> ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path} (epoch {epoch_num})")

        # ===================== í—¬í¼ë“¤ =====================

        def _strip_special(ids, BOS, EOS, PAD):
            if torch.is_tensor(ids): ids = ids.tolist()
            if ids and ids[0] == BOS: ids = ids[1:]
            if EOS in ids:
                ids = ids[:ids.index(EOS)]
            return [t for t in ids if t != PAD]

        def _first_token_text(ids):
            """ì²« ë¹„-íŠ¹ìˆ˜í† í°ì„ í…ìŠ¤íŠ¸ë¡œ (ìƒ˜í”Œ ì¶œë ¥ìš©)"""
            seq = ids.tolist() if torch.is_tensor(ids) else list(ids)
            first = ""
            for t in seq:
                if t in (self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID): continue
                try:
                    first = safe_decode(tokenizer, [t])
                except:
                    first = ""
                break
            return first

        def _length_penalty(length, alpha=length_alpha):
            # GNMT length penalty
            return ((5 + length) ** alpha) / ((5 + 1) ** alpha)

        def _apply_repetition_penalty(logits_row, generated, penalty):
            # CTRL ë°©ì‹: ì´ë¯¸ ìƒì„±ëœ í† í°ì˜ logitì„ ì¡°ì •
            if penalty == 1.0 or len(generated) == 0:
                return logits_row
            unique_tokens = set(generated)
            # in-place ì „í™˜ ì „ clone
            logits_row = logits_row.clone()
            with torch.no_grad():
                for t in unique_tokens:
                    if t < 0 or t >= logits_row.numel():  # ì•ˆì „ì¥ì¹˜
                        continue
                    val = logits_row[t]
                    logits_row[t] = val / penalty if val > 0 else val * penalty
            return logits_row

        def _banned_tokens_for_ngram(seq, n):
            # no-repeat n-gram: ë§ˆì§€ë§‰ n-1 í† í° prefixì™€ ê³¼ê±° next-tokenë“¤ì„ ê¸ˆì§€
            if n <= 0 or len(seq) < n - 1:
                return set()
            banned = set()
            prefix = tuple(seq[-(n - 1):]) if n - 1 > 0 else tuple()
            # build n-gram dict
            hist = {}
            for i in range(len(seq) - n + 1):
                gram = tuple(seq[i:i + n])
                pfx = gram[:-1]
                nxt = gram[-1]
                hist.setdefault(pfx, set()).add(nxt)
            if prefix in hist:
                banned = hist[prefix]
            return banned

        @torch.no_grad()
        def beam_search_generate(src_ids_b, src_len_b):
            """
            src_ids_b : (Tsrc,) Long
            src_len_b : () Long
            return: List[int] (BOS ... EOS)
            """
            beams = [ (0.0, [self.BOS_TOKEN_ID]) ]  # (cum_logprob, seq)

            for step in range(max_gen_len):
                new_beams = []
                all_ended = True
                for score, seq in beams:
                    if seq[-1] == self.EOS_TOKEN_ID:
                        new_beams.append((score, seq))
                        continue
                    all_ended = False

                    # ë””ì½”ë” ì…ë ¥ êµ¬ì„±
                    dec_inp = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)
                    # ëª¨ë¸ forward (ê°„ë‹¨êµ¬í˜„: ì „ì²´ ê¸¸ì´ ì¬ì‹¤í–‰)
                    logits = model(
                        src_ids_b.unsqueeze(0),         # (1, Tsrc)
                        src_len_b.unsqueeze(0),         # (1,)
                        dec_inp                         # (1, t)
                    )                                   # (1, t, V)
                    next_logits = logits[:, -1, :].squeeze(0)  # (V,)

                    # ë°˜ë³µ íŒ¨ë„í‹°
                    next_logits = _apply_repetition_penalty(next_logits, seq, repetition_penalty)

                    # no-repeat n-gram ê¸ˆì§€ í† í° -inf ì²˜ë¦¬
                    banned = _banned_tokens_for_ngram(seq, no_repeat_ngram_size)
                    if banned:
                        next_logits[list(banned)] = float('-inf')

                    # íŒ¨ë”©ì€ ìƒì„± ê¸ˆì§€
                    next_logits[self.PAD_TOKEN_ID] = float('-inf')

                    logprobs = F.log_softmax(next_logits, dim=-1)  # (V,)
                    topk_logprobs, topk_ids = torch.topk(logprobs, beam_size)

                    for lp, idx in zip(topk_logprobs.tolist(), topk_ids.tolist()):
                        new_seq = seq + [idx]
                        new_score = score + lp  # raw logprob ëˆ„ì 
                        new_beams.append((new_score, new_seq))

                if all_ended:
                    break

                # ë¹” ì •ë ¬ ë° ìƒìœ„ K ìœ ì§€
                new_beams.sort(key=lambda x: x[0], reverse=True)
                beams = new_beams[:beam_size]

            # ì™„ê²° ë¹” ì„ íƒ(ê¸¸ì´ íŒ¨ë„í‹° ë°˜ì˜)
            def finalized_score(b):
                sc, seq = b
                eff_len = len(seq) if self.EOS_TOKEN_ID not in seq else (seq.index(self.EOS_TOKEN_ID) + 1)
                return sc / _length_penalty(eff_len, length_alpha)

            best = max(beams, key=finalized_score)
            return best[1]

        # ===================== í‰ê°€ ë£¨í”„ =====================

        model.eval()

        total_tp = total_fp = total_fn = 0
        num_samples = 0

        # (ì˜µì…˜) TF-loss/acc ì§„ë‹¨
        tf_total_loss = 0.0
        tf_total_tok  = 0
        tf_correct_tok = 0

        # (ì˜µì…˜) ERRANT ë¤í”„ ì¤€ë¹„
        dump_src, dump_hyp, dump_ref = [], [], []

        # ìƒ˜í”Œ ì¶œë ¥(ì…ë ¥/ì˜ˆì¸¡/ì •ë‹µ) 5ê°œë§Œ
        printed_samples = 0
        MAX_PRINT = 5

        pbar = tqdm(val_loader, desc=f"Validation (beam={beam_size}, Î±={length_alpha}, rep={repetition_penalty}, ngr={no_repeat_ngram_size})")
        for batch in pbar:
            input_ids  = batch['input_ids'].to(device)   # (B, T)
            output_ids = batch['output_ids'].to(device)  # (B, T)
            input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1)  # (B,)

            B = input_ids.size(0)
            # 1) í”„ë¦¬ëŸ¬ë‹ ìƒì„± (beam search) â€” ìƒ˜í”Œ ë‹¨ìœ„
            preds = []
            for b in range(B):
                hyp_ids = beam_search_generate(input_ids[b], input_lengths[b])
                preds.append(hyp_ids)

            # 2) ì •ë ¬ ê¸°ë°˜ í¸ì§‘ì§€í‘œ(ê·¼ì‚¬) â€” ë§ˆì´í¬ë¡œ í‰ê· 
            for b in range(B):
                src_seq = _strip_special(input_ids[b],  self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                ref_seq = _strip_special(output_ids[b], self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)
                hyp_seq = _strip_special(torch.tensor(preds[b], device=device), self.BOS_TOKEN_ID, self.EOS_TOKEN_ID, self.PAD_TOKEN_ID)

                sm_gold = difflib.SequenceMatcher(a=src_seq, b=ref_seq)
                gold_spans = [op for op in sm_gold.get_opcodes() if op[0] != 'equal']

                sm_pred = difflib.SequenceMatcher(a=src_seq, b=hyp_seq)
                pred_spans = [op for op in sm_pred.get_opcodes() if op[0] != 'equal']

                # src ê¸°ì¤€ span overlapìœ¼ë¡œ TP ê·¼ì‚¬
                tp = 0
                for tag, gi1, gi2, gj1, gj2 in gold_spans:
                    for tag2, pi1, pi2, pj1, pj2 in pred_spans:
                        if not (pi2 <= gi1 or pi1 >= gi2):  # overlap on src
                            tp += 1
                            break
                fp = max(0, len(pred_spans) - tp)
                fn = max(0, len(gold_spans) - tp)

                total_tp += tp
                total_fp += fp
                total_fn += fn

                # 3) ERRANT ë¤í”„ í…ìŠ¤íŠ¸ ì €ì¥
                if dump_for_errant:
                    src_txt = safe_decode(tokenizer, [self.BOS_TOKEN_ID] + src_seq + [self.EOS_TOKEN_ID])
                    ref_txt = safe_decode(tokenizer, [self.BOS_TOKEN_ID] + ref_seq + [self.EOS_TOKEN_ID])
                    hyp_txt = safe_decode(tokenizer, preds[b])

                    dump_src.append(src_txt)
                    dump_ref.append(ref_txt)
                    dump_hyp.append(hyp_txt)

                # 4) ìƒ˜í”Œ ì¶œë ¥(ìµœëŒ€ 5ê°œ, trainê³¼ ë™ì¼ ìŠ¤íƒ€ì¼)
                if printed_samples < MAX_PRINT:
                    input_text  = safe_decode(tokenizer, input_ids[b].detach().cpu().tolist())
                    pred_text   = safe_decode(tokenizer, preds[b])
                    output_text = safe_decode(tokenizer, output_ids[b].detach().cpu().tolist())

                    # ì²« í† í° ë¹„êµ(ë¹„-íŠ¹ìˆ˜)
                    pred_first  = _first_token_text(preds[b])
                    gold_first  = _first_token_text(output_ids[b])

                    print(f"> ì²« í† í° ë¹„êµ | ì˜ˆì¸¡: {pred_first} / ì •ë‹µ: {gold_first}\n")
                    print(f"\tìƒ˜í”Œ {printed_samples+1}:")
                    print(f"\t> ì…ë ¥: {input_text}")
                    print(f"\t> ì˜ˆì¸¡: {pred_text}")
                    print(f"\t> ì •ë‹µ: {output_text}")
                    printed_samples += 1

            num_samples += B

            # 5) (ì˜µì…˜) TF-loss/acc ì§„ë‹¨ â€” ë¹ ë¥´ê²Œ ë³´ê³  ì‹¶ì„ ë•Œë§Œ
            if diag_tf:
                with torch.no_grad():
                    dec_inp = output_ids[:, :-1]
                    target  = output_ids[:,  1:]
                    logits  = model(input_ids, input_lengths, dec_inp)  # (B, T-1, V)
                    logits_f = logits.view(-1, logits.size(-1))
                    target_f = target.contiguous().view(-1)
                    loss = criterion(logits_f, target_f)
                    nonpad = (target_f != self.PAD_TOKEN_ID).sum().item()
                    tf_total_loss += loss.item() * nonpad
                    tf_total_tok  += nonpad

                    pred_tf = logits.argmax(dim=-1)  # (B, T-1)
                    tf_correct_tok += ((pred_tf == target) & (target != self.PAD_TOKEN_ID)).sum().item()

        # --------- ì§‘ê³„ ---------
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        recall    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        beta = 0.5
        f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0

        print(f"\n>> Validation ê²°ê³¼ (epoch {epoch_num}) [beam={beam_size}, Î±={length_alpha}, rep={repetition_penalty}, ngr={no_repeat_ngram_size}]")
        print(f"   Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")

        if diag_tf and tf_total_tok > 0:
            tf_avg_loss = tf_total_loss / tf_total_tok
            tf_acc = tf_correct_tok / tf_total_tok
            print(f"   (TF ì§„ë‹¨) Avg Loss: {tf_avg_loss:.4f}, Token Acc: {tf_acc:.4f}")

        # --------- ERRANT ë¤í”„ ì €ì¥ ---------
        if dump_for_errant:
            dump_root = os.path.join(transformer_path, dump_dir_name, f"epoch_{epoch_num if epoch_num>=0 else 'NA'}")
            os.makedirs(dump_root, exist_ok=True)
            src_path = os.path.join(dump_root, "src.txt")
            hyp_path = os.path.join(dump_root, "hyp.txt")
            ref_path = os.path.join(dump_root, "ref.txt")

            with open(src_path, "w", encoding="utf-8") as f:
                f.write("\n".join(dump_src))
            with open(hyp_path, "w", encoding="utf-8") as f:
                f.write("\n".join(dump_hyp))
            with open(ref_path, "w", encoding="utf-8") as f:
                f.write("\n".join(dump_ref))

            print(f"\n[ERRANT ë¤í”„] ì €ì¥ ì™„ë£Œ:")
            print(f"  SRC: {src_path}")
            print(f"  HYP: {hyp_path}")
            print(f"  REF: {ref_path}")
            print("  â†’ ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ë¡œ ERRANT ì±„ì  ì‹¤í–‰ ê¶Œì¥ (ì–¸ì–´ ë¦¬ì†ŒìŠ¤/í† í¬ë‚˜ì´ì € ì¤€ë¹„ í•„ìš”)")


if __name__ == '__main__':
    tne = pNup_s2s()
    tne.train()
    tne.evaluate()