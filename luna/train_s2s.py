# Luna ì‚¬ì „í•™ìŠµ + ë¯¸ì„¸ì¡°ì • í…œí”Œë¦¿ (Gradient Checkpointing + FP16 + DeepSpeed ZeRO)

"""
# $ìˆ˜ì •í•„. trainê³¼ evaluate ëª¨ë‘ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ í†µí•©.
# í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ ê³³ì—ì„œ ì´ˆê¸°í™” í•˜ê³  ê¸°ëŠ¥ì ì¸ ë¶€ë¶„ì€ ë¶„ë¦¬.
# ê° í•¨ìˆ˜ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ì˜ ë³€ë™ê°’ì— ì£¼ì˜.
"""

import os, sys, csv
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
from tqdm import tqdm

drive_path = "/workspace"
# Colab í™˜ê²½ì—ì„œëŠ” drive_pathë¡œ ì´ë™ì´ í•„ìš” -> ì•„ë˜ì˜ í´ë˜ìŠ¤í‹€ì„ import í•˜ê¸° ìœ„í•¨
#%cd "$drive_path"

from transformer.luna.model import LunaTransformerEncoder, EditBasedLunaModel, LunaTransformer
from transformer.code_transformer.dataset import SpellingDataset
from transformer.code_transformer.sentencepiece_tokenizer import SentencePieceTokenizer
from transformer.code_transformer.WeightedCELossForGEC import WeightedCELossForGEC
from transformer.code_transformer.EditBasedLoss import EditBasedLoss
from transformer.code_transformer.EditBasedDecoder import EditBasedDecoder

# ------------------------
# ì•ˆì „í•œ ë””ì½”ë”© í•¨ìˆ˜
# ------------------------
def safe_decode(tokenizer, ids):
    try:
        # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ID í•„í„°ë§
        vocab_size = tokenizer.get_piece_size()
        valid_ids = [id for id in ids if id < vocab_size]

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if not valid_ids:
            return "<ë¹ˆ ì‹œí€€ìŠ¤>"

        return tokenizer.decode(valid_ids)
    except Exception as e:
        return f"ë””ì½”ë”© ì˜¤ë¥˜: {e}"


# ------------------------
# ê°€ì¥ ìµœì‹ ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ path ë°˜í™˜
# ------------------------
def get_latest_checkpoint(checkpoint_dir):
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
    if not checkpoints:
        raise FileNotFoundError("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
    return os.path.join(checkpoint_dir, latest_checkpoint)

# ------------------------
# 1. Luna ëª¨ë¸ ì •ì˜
# ------------------------
class LunaModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_attention_heads=8):
        super().__init__()
        self.encoder = LunaTransformerEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            num_layers=num_layers,
            num_attention_heads=num_attention_heads,
            d_ff=2048,
            dropout_p=0.2,
            project_embedding_length=32,
            max_length=1024
        )
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, input_lengths):
        # ì¸ì½”ë”ë¥¼ í†µê³¼
        encoder_outputs = self.encoder(input_ids, input_lengths)
        # ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ í†µê³¼
        logits = self.lm_head(encoder_outputs)
        return logits

# ------------------------
# 2. í•™ìŠµ ì„¤ì •
# ------------------------
def label_smoothed_loss(pred, target, epsilon=0.1):
    n_class = pred.size(-1)
    log_probs = F.log_softmax(pred, dim=-1)
    one_hot = F.one_hot(target, num_classes=n_class).float()
    smoothed_target = (1 - epsilon) * one_hot + epsilon / n_class
    loss = -(smoothed_target * log_probs).sum(dim=-1)
    return loss.mean()

class pNup_s2s:
    def __init__(self):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        self.BATCH_SIZE = 32
        self.EPOCHS = 25
        self.LEARNING_RATE = 0.0001
        self.D_MODEL = 512
        self.NUM_HEADS = 8
        self.NUM_LAYERS = 6
        self.D_FF = 2048
        self.MAX_SEQ_LENGTH = 128
        self.DROPOUT = 0.2
        self.VOCAB_SIZE = 16000
        self.PAD_TOKEN_ID = 0    # íŒ¨ë”© í† í° ID
        self.BOS_TOKEN_ID = 2    # ì‹œì‘ í† í° ID( <s> or [BOS] )
        self.EOS_TOKEN_ID = 3    # ì¢…ë£Œ í† í° ID

    def train(self):
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •
        transformer_path = "/workspace/transformer"
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')
        tokenizer = SentencePieceTokenizer(train_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        train_loader = DataLoader(dataset.train_dataset, batch_size=self.BATCH_SIZE, shuffle=True)
        checkpoints = [f for f in os.listdir(f"{drive_path}/transformer/checkpoints") if f.endswith('.pt')]
        
        # ëª¨ë¸ ê°ì²´ ìƒì„±
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=self.D_MODEL,
            num_layers=self.NUM_LAYERS,
            num_attention_heads=self.NUM_HEADS,
            d_ff = self.D_FF,
            dropout_p = self.DROPOUT,
            project_embedding_length = 32,
            max_length = self.MAX_SEQ_LENGTH
        )
        model = model.to(device)

        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        criterion = nn.CrossEntropyLoss(ignore_index=self.PAD_TOKEN_ID)
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.LEARNING_RATE, weight_decay=0.01)
        scaler = GradScaler()  # FP16ì„ ìœ„í•œ Gradient Scaler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=2, verbose=True
        )

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        if not checkpoints:
            print("ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ.")
            latest_checknum = 0
        else:
            # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint_path = get_latest_checkpoint(f"{drive_path}/transformer/checkpoints")
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

            # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
            latest_checkpoint = os.path.basename(checkpoint_path)
            latest_checknum = int(latest_checkpoint.split('_')[-1].split('.')[0])

            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            model = model.to(device)

        # í•™ìŠµ ë£¨í”„
        model.train()
        for epoch in range(latest_checknum, self.EPOCHS + latest_checknum):
            total_loss = 0
            total_edit_ratio = 0
            
            total_gold_edits = 0
            total_pred_edits = 0
            correct_edit_total = 0
            correct_tokens = 0
            total_tokens = 0
            
            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.EPOCHS+latest_checknum}')
            
            for batch_idx, batch in enumerate(progress_bar):
                # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
                input_ids = batch['input_ids'].to(device)
                output_ids = batch['output_ids'].to(device)
                
                # input_lengths ê³„ì‚° (íŒ¨ë”© í† í° 0ì„ ì œì™¸í•œ ì‹¤ì œ ê¸¸ì´)
                input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1).to(device)

                decoder_input = output_ids[:, :-1]  # Decoder ì…ë ¥
                target = output_ids[:, 1:]          # ì •ë‹µ
                optimizer.zero_grad()

                ### ë””ë²„ê¹…ìš© ì„ì‹œ ë³€ê²½ : autocast ì œê±°
                #with autocast():  # ìë™ í˜¼í•© ì •ë°€ë„ (FP16)      
                outputs = model(input_ids, input_lengths, decoder_input)
                outputs = outputs.view(-1, outputs.size(-1))
                target = target.contiguous().view(-1)
                """
                outputs = model(input_ids, input_lengths, output_ids)
                outputs = outputs.view(-1, outputs.size(-1))  # (batch_size * seq_len, vocab_size)
                target = output_ids.view(-1)  # (batch_size * seq_len)
                """
                if not torch.isfinite(outputs).all():
                    print("[Debug]ë°°ì¹˜ë³„ target ë¹„-íŒ¨ë”© í† í° ìˆ˜:", (target != self.PAD_TOKEN_ID).sum(dim=1).tolist())
                    print("[Debug]Decoder outputs shape:", outputs.shape)
                    print("[Debug]Target shape:", target.shape)
                    print("[DebugğŸš¨] outputs í…ì„œ ë‚´ NaN/Inf ì¡´ì¬!")
                    print("ì˜ˆì‹œ ì¶œë ¥ (ì²« 5ê°œ):", outputs[0][:5])
                    print("ìµœëŒ€ê°’:", outputs.max().item(), "ìµœì†Œê°’:", outputs.min().item(), "í‰ê· ê°’:", outputs.mean().item())

                #loss = criterion(outputs, target)
                loss = label_smoothed_loss(outputs, target)

                # FP16 í•™ìŠµì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ëœ ì—­ì „íŒŒ
                """
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                """
                # Gradient Clipping
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()


                # ìˆ˜ì • ë¹„ìœ¨ ê³„ì‚°
                edit_ratio = ((output_ids != input_ids) & (output_ids != 0)).float().mean().item()
                total_edit_ratio += edit_ratio

                total_loss += loss.item()
                if not torch.isfinite(loss):
                    print(f"[DebugğŸš¨] ë¹„ì •ìƒ Loss ë°œìƒ: {loss.item()}")
                    print("[Debug]í˜„ì¬ learning rate:", scheduler.optimizer.param_groups[0]['lr'])
                
                # ì˜ˆì¸¡
                pred_ids = outputs.argmax(dim=-1)
                pred_ids = pred_ids.view(output_ids.size(0), output_ids.size(1) - 1)

                # ìˆ˜ì • ë¹„ìœ¨
                edit_ratio = ((output_ids != input_ids) & (output_ids != self.PAD_TOKEN_ID)).float().mean().item()
                total_edit_ratio += edit_ratio

                # ì •í™•ë„ ê³„ì‚°ìš©
                target_2d = target.view(pred_ids.size(0), pred_ids.size(1))

                # ì •í™•ë„ ê³„ì‚°
                correct_tokens += ((pred_ids == target_2d) & (target_2d != self.PAD_TOKEN_ID)).sum().item()
                total_tokens += (target_2d != self.PAD_TOKEN_ID).sum().item()

                # í¸ì§‘ ì •í™•ë„
                gold_edits = ((output_ids[:, 1:] != input_ids[:, 1:]) & (output_ids[:, 1:] != self.PAD_TOKEN_ID))
                pred_edits = ((pred_ids != input_ids[:, 1:]) & (pred_ids != self.PAD_TOKEN_ID))
                correct_edits = ((pred_ids == output_ids[:, 1:]) & gold_edits)

                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()
                correct_edit_total += correct_edits.sum().item()
                
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    #'edit_ratio': f'{edit_ratio:.2f}'
                })
                
                # ìƒ˜í”Œ ì¶œë ¥ (ê° ì—í¬í¬ë§ˆë‹¤ 5ê°œ)
                if batch_idx < 5:
                    try:
                        input_text = safe_decode(tokenizer, input_ids[0].cpu().tolist())
                        output_text = safe_decode(tokenizer, output_ids[0].cpu().tolist())
                        pred_ids_with_pad = [self.BOS_TOKEN_ID] + pred_ids[0].cpu().tolist()
                        pred_text = safe_decode(tokenizer, pred_ids_with_pad)

                        # === ì²« í† í° ë¹„êµ ë””ë²„ê·¸ ì¶”ê°€ ===
                        pred_first = safe_decode(tokenizer, [pred_ids[0, 0].item()])
                        gold_first = safe_decode(tokenizer, [output_ids[0, 1].item()])  # [BOS] ë‹¤ìŒ í† í°
                        print(f"\t> ì²« í† í° ë¹„êµ | ì˜ˆì¸¡: {pred_first} / ì •ë‹µ: {gold_first}")
                        # =============================

                        # ìƒ˜í”Œ ì¶œë ¥
                        print(f"\tìƒ˜í”Œ {batch_idx+1}:")
                        print(f"\t> ì…ë ¥: {input_text}")
                        print(f"\t> ì˜ˆì¸¡: {pred_text}")
                        print(f"\t> ì •ë‹µ: {output_text}")

                    except Exception as e:
                        print(f"[ErrorğŸš¨] ìƒ˜í”Œ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            avg_loss = total_loss / len(train_loader)
            avg_edit_ratio = total_edit_ratio / len(train_loader)
            
            token_acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0

            precision = correct_edit_total / total_pred_edits if total_pred_edits > 0 else 0.0
            recall = correct_edit_total / total_gold_edits if total_gold_edits > 0 else 0.0
            beta = 0.5
            if precision + recall > 0:
                f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)
            else:
                f0_5 = 0.0

            print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}, Avg Edit Ratio: {avg_edit_ratio:.4f}")
            print(f"   Token Acc: {token_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")
            #print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Average Edit Ratio: {avg_edit_ratio:.4f}")
            
            with open(f"{transformer_path}\\epoch_metrics.csv", mode="a", newline="") as file:
                writer = csv.writer(file)
                if epoch == 0:
                    writer.writerow(["epoch", "loss", "edit_ratio", "token_acc", "precision", "recall", "f0.5"])
                writer.writerow([
                    epoch + 1,
                    avg_loss,
                    avg_edit_ratio,
                    token_acc,
                    precision,
                    recall,
                    f0_5
                ])

            # í•™ìŠµë¥  ì¡°ì •
            scheduler.step(avg_loss)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if((epoch + 1) % 5 == 0):
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                    #'edit_ratio': avg_edit_ratio,
                    'tokenizer': tokenizer
                }
                torch.save(checkpoint, f"{drive_path}/transformer/checkpoints/luna_model_epoch_{epoch+1}.pt")

    # ------------------------
    # 3. í‰ê°€ ì„¤ì •
    # ------------------------
    def evaluate(self):
        import os, torch
        from torch.utils.data import DataLoader
        from tqdm import tqdm
        import torch.nn.functional as F

        # ===== ê¸°ë³¸ ì„¤ì • =====
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        CHECKPOINT_DIR   = f"{drive_path}/transformer/checkpoints"
        transformer_path = f"{drive_path}/transformer"
        train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
        val_path   = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')

        # ===== í† í¬ë‚˜ì´ì €/ë°ì´í„°ì…‹ =====
        tokenizer = SentencePieceTokenizer(val_path, vocab_size=self.VOCAB_SIZE, max_length=self.MAX_SEQ_LENGTH).tokenizer
        dataset = SpellingDataset(train_path, val_path, tokenizer, self.MAX_SEQ_LENGTH)
        val_loader = DataLoader(dataset.val_dataset, batch_size=self.BATCH_SIZE, shuffle=False)

        # ===== ëª¨ë¸/criterion =====
        model = LunaTransformer(
            vocab_size=self.VOCAB_SIZE,
            d_model=self.D_MODEL,
            num_layers=self.NUM_LAYERS,
            num_attention_heads=self.NUM_HEADS,
            d_ff=self.D_FF,
            dropout_p=self.DROPOUT,
            project_embedding_length=32,
            max_length=self.MAX_SEQ_LENGTH
        ).to(device)

        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.PAD_TOKEN_ID)

        # ===== ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ =====
        ckpt_path = get_latest_checkpoint(CHECKPOINT_DIR)
        if ckpt_path is None:
            print(">> ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", CHECKPOINT_DIR)
            return
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        latest_checkpoint = os.path.basename(ckpt_path)
        epoch_num = int(latest_checkpoint.split('_')[-1].split('.')[0])
        print(f">> ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path} (epoch {epoch_num})")

        # ==== [ì¶”ê°€] free-running ìƒì„± í•¨ìˆ˜ ====
        @torch.no_grad()
        def free_run_generate(model, input_ids, input_lengths,
                            max_len, bos_id, eos_id, pad_id):
            """
            ì…ë ¥ ë°°ì¹˜ì— ëŒ€í•´ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œë¡œ ë””ì½”ë”©(teacher-forcing ì—†ìŒ).
            ë°˜í™˜: pred_full (B, T_full) : [BOS] ... EOS ... PAD
            """
            B = input_ids.size(0)
            # ì‹œì‘: [BOS] + PAD...  í˜•íƒœë¡œ ì´ˆê¸°í™”
            dec = torch.full((B, 1), bos_id, dtype=input_ids.dtype, device=input_ids.device)

            # ì´ë¯¸ EOSë¥¼ ë‚¸ ìƒ˜í”Œ ë§ˆìŠ¤í¬
            finished = torch.zeros(B, dtype=torch.bool, device=input_ids.device)

            # ìµœëŒ€ ê¸¸ì´-1 ë§Œí¼ ë°˜ë³µ (BOS í¬í•¨í•˜ë¯€ë¡œ -1)
            for t in range(1, max_len):
                # ëª¨ë¸ì€ ì „ì²´ decë¥¼ ë°›ì•„ (B, t, V) ë¡œì§“ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
                logits = model(input_ids, input_lengths, dec)  # (B, t, V)
                next_ids = logits[:, -1, :].argmax(dim=-1)     # ë§ˆì§€ë§‰ stepì˜ ì˜ˆì¸¡ (B,)

                # ì•„ì§ ëë‚˜ì§€ ì•Šì€ ìœ„ì¹˜ë§Œ ì—…ë°ì´íŠ¸
                next_ids = torch.where(finished, torch.full_like(next_ids, pad_id), next_ids)

                # EOSê°€ ìƒˆë¡œ ìƒì„±ëœ ìœ„ì¹˜ ê°±ì‹ 
                finished |= (next_ids == eos_id)

                # ì‹œí€€ìŠ¤ì— ë¶™ì´ê¸°
                dec = torch.cat([dec, next_ids.unsqueeze(1)], dim=1)

                # ë°°ì¹˜ê°€ ëª¨ë‘ EOSë©´ ì¡°ê¸° ì¢…ë£Œ
                if finished.all():
                    break

            # ê¸¸ì´ ëª¨ìë¼ë©´ PADë¡œ ìš°ì¸¡ íŒ¨ë”©
            if dec.size(1) < max_len:
                pad_cols = max_len - dec.size(1)
                pad = torch.full((B, pad_cols), pad_id, dtype=dec.dtype, device=dec.device)
                dec = torch.cat([dec, pad], dim=1)

            return dec  # (B, max_len)

        # ===== í‰ê°€ ë£¨í”„ =====
        model.eval()
        total_loss = 0.0
        total_tokens = 0
        correct_tokens = 0

        total_gold_edits = 0
        total_pred_edits = 0
        correct_edits = 0

        first_batch_guard_printed = False

        # ìƒ˜í”Œ ì¶œë ¥ ê°œìˆ˜ ì»¨íŠ¸ë¡¤ (ì›í•˜ë©´ 3ìœ¼ë¡œ ë°”ê¿”ë„ ë¨)
        N_SAMPLES = 5
        printed_examples = 0

        # ë””ì½”ë”©ìš© í•¨ìˆ˜: PAD ì œê±° + BOS/EOS ì˜ë¼ë‚´ê¸°
        def _strip_special(ids, bos_id=self.BOS_TOKEN_ID, eos_id=self.EOS_TOKEN_ID, pad_id=self.PAD_TOKEN_ID):
            # ë¦¬ìŠ¤íŠ¸/í…ì„œ ëª¨ë‘ ì§€ì›
            if torch.is_tensor(ids):
                ids = ids.tolist()
            # ì•ìª½ BOS ì œê±°
            if ids and ids[0] == bos_id:
                ids = ids[1:]
            # EOS ì´í›„ ì˜ë¼ë‚´ê¸°
            if eos_id in ids:
                cut = ids.index(eos_id)
                ids = ids[:cut]
            # PAD ì œê±°
            ids = [t for t in ids if t != pad_id]
            return ids

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids  = batch['input_ids'].to(device)     # (B, T_full)
                output_ids = batch['output_ids'].to(device)    # (B, T_full) = [BOS ... EOS PAD...]

                input_lengths = (input_ids != self.PAD_TOKEN_ID).sum(dim=1)

                # ---- (A) teacher-forcing ê¸°ë°˜ lossë§Œ ê³„ì‚° (ì§„ë‹¨ìš©) ----
                decoder_input = output_ids[:, :-1].contiguous()
                target        = output_ids[:,  1:].contiguous()   # (B, T-1)

                logits = model(input_ids, input_lengths, decoder_input)  # (B, T-1, V)

                if not first_batch_guard_printed:
                    # ì •ë ¬/shape ê°€ë“œ
                    eos_pos = (output_ids[0] != self.PAD_TOKEN_ID).sum() - 1
                    print("[VAL] BOS/EOS (should be 2,3):", output_ids[0,0].item(), output_ids[0, eos_pos].item())
                    print("[VAL] shapes (logits vs target):", logits.shape, target.shape)
                    print("[VAL] non-pad ratio in target:", (target != self.PAD_TOKEN_ID).float().mean().item())
                    print("[VAL] sample target head:", target[0, :10].tolist())
                    print("[VAL] sample pred head  :", logits.argmax(-1)[0, :10].tolist())
                    print("[VAL] sample decoder_inp:", decoder_input[0, :10].tolist())
                    first_batch_guard_printed = True

                loss = criterion(logits.view(-1, self.VOCAB_SIZE), target.view(-1))
                non_pad_mask = (target != self.PAD_TOKEN_ID)
                num_non_pad = non_pad_mask.sum().item()
                total_loss += loss.item() * num_non_pad
                total_tokens += num_non_pad

                # ---- (Eâ€™) free-running ìƒì„±ìœ¼ë¡œ ì˜ˆì¸¡/ì§€í‘œ ê³„ì‚° ----
                pred_full = free_run_generate(
                    model=model,
                    input_ids=input_ids,
                    input_lengths=input_lengths,
                    max_len=output_ids.size(1),   # ì •ë‹µ ê¸¸ì´ì— ë§ì¶° ìƒì„±
                    bos_id=self.BOS_TOKEN_ID,
                    eos_id=self.EOS_TOKEN_ID,
                    pad_id=self.PAD_TOKEN_ID
                )  # (B, T_full)

                # í† í° ì •í™•ë„ëŠ” free-running ì˜ˆì¸¡ ê¸°ì¤€ìœ¼ë¡œ(ì‹¤ì‚¬ìš©ê³¼ ì¼ì¹˜)
                non_pad_full = (output_ids != self.PAD_TOKEN_ID)
                correct_tokens += ((pred_full == output_ids) & non_pad_full).sum().item()

                # í¸ì§‘ ì§€í‘œ(ì…ë ¥/ì •ë‹µ/ì˜ˆì¸¡ì„ ê°™ì€ í”„ë ˆì„ì—ì„œ ë¹„êµ)
                valid_mask = (output_ids != self.PAD_TOKEN_ID) & (output_ids != self.BOS_TOKEN_ID) & (output_ids != self.EOS_TOKEN_ID)
                gold_edits = (output_ids != input_ids) & valid_mask
                pred_edits = (pred_full  != input_ids) & valid_mask

                correct_edits   += ((pred_full == output_ids) & gold_edits).sum().item()
                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()

                # ë°°ì¹˜ì—ì„œ ëª‡ ê°œ ë½‘ì•„ "ì…ë ¥/ì˜ˆì¸¡/ì •ë‹µ + ì²« í† í° ë¹„êµ" ì¶œë ¥
                if printed_examples < N_SAMPLES:
                    bsz = input_ids.size(0)
                    # ê°€ëŠ¥í•œ ë§Œí¼ë§Œ ì¶œë ¥
                    take = min(N_SAMPLES - printed_examples, bsz)
                    for i in range(take):
                        # ì²« í† í° ë¹„êµ(ì˜ˆì¸¡ì€ pred_full[:,1], ì •ë‹µì€ output_ids[:,1])
                        try:
                            pred_first_id = pred_full[i, 1].item()
                            gold_first_id = output_ids[i, 1].item()
                            pred_first = safe_decode(tokenizer, [pred_first_id])
                            gold_first = safe_decode(tokenizer, [gold_first_id])
                            print(f"> ì²« í† í° ë¹„êµ | ì˜ˆì¸¡: {pred_first} / ì •ë‹µ: {gold_first}")
                        except Exception as e:
                            print(f"> ì²« í† í° ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")

                        # ë³¸ë¬¸ ë””ì½”ë”©(íŠ¹ìˆ˜í† í°/íŒ¨ë”© ì œê±°)
                        in_ids   = _strip_special(input_ids[i])
                        pr_ids   = _strip_special(pred_full[i])
                        out_ids  = _strip_special(output_ids[i])

                        input_text  = safe_decode(tokenizer, in_ids)
                        pred_text   = safe_decode(tokenizer, pr_ids)
                        output_text = safe_decode(tokenizer, out_ids)

                        print(f"\n\tìƒ˜í”Œ {printed_examples + 1}:")
                        print(f"\t> ì…ë ¥: {input_text}")
                        print(f"\t> ì˜ˆì¸¡: {pred_text}")
                        print(f"\t> ì •ë‹µ: {output_text}")

                        printed_examples += 1
                        if printed_examples >= N_SAMPLES:
                            break

        # ===== ì§€í‘œ ì§‘ê³„ =====
        if total_tokens > 0:
            avg_loss  = total_loss / total_tokens        # teacher-forcing loss (ì§„ë‹¨ìš©)
        else:
            avg_loss = float('nan')

        # free-running ê¸°ì¤€ í† í° ì •í™•ë„/í¸ì§‘ ì§€í‘œ
        total_eval_tokens = ( (dataset.val_dataset[0]['output_ids']).__len__() )  # ì‚¬ìš© ì•ˆ í•¨: ì•„ë˜ì—ì„œ non_pad_full í•©ìœ¼ë¡œ ê³„ì‚°í–ˆìŒ
        token_acc = correct_tokens / ( (non_pad_full).sum().item() )  # ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ë§ˆìŠ¤í¬ê°€ ì•„ë‹ˆë¼ ì „ì²´ í•©ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ ì •í™•í•˜ì§€ë§Œ,
                                                                    # ìœ„ì—ì„œ ë°°ì¹˜ë³„ë¡œ ëˆ„ì í•´ë„ ë™ì¼í•©ë‹ˆë‹¤.

        precision = (correct_edits / total_pred_edits) if total_pred_edits > 0 else 0.0
        recall    = (correct_edits / total_gold_edits) if total_gold_edits > 0 else 0.0
        beta = 0.5
        f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0

        print(f"\n>> Validation ê²°ê³¼ (epoch {epoch_num}) [free-running]:")
        print(f"   (TF-loss ì§„ë‹¨) í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        print(f"   Token Acc: {token_acc:.4f}")
        print(f"   Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")

if __name__ == '__main__':
    tne = pNup_s2s()
    #train()
    tne.evaluate()

