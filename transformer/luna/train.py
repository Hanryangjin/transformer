# Luna ì‚¬ì „í•™ìŠµ + ë¯¸ì„¸ì¡°ì • í…œí”Œë¦¿ (Gradient Checkpointing + FP16 + DeepSpeed ZeRO)

import os, sys, csv
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
from tqdm import tqdm

drive_path = "/workspace"
# Colab í™˜ê²½ì—ì„œëŠ” drive_pathë¡œ ì´ë™ì´ í•„ìš” -> ì•„ë˜ì˜ í´ë˜ìŠ¤í‹€ì„ import í•˜ê¸° ìœ„í•¨
#%cd "$drive_path"

from transformer.luna.model import LunaTransformerEncoder, EditBasedLunaModel, LunaTransformer
from transformer.code_transformer.dataset import SpellingDataset
from transformer.code_transformer.sentencepiece_tokenizer import SentencePieceTokenizer
from transformer.code_transformer.WeightedCELossForGEC import WeightedCELossForGEC
from transformer.code_transformer.EditBasedLoss import EditBasedLoss
from transformer.code_transformer.EditBasedDecoder import EditBasedDecoder

# ------------------------
# ì•ˆì „í•œ ë””ì½”ë”© í•¨ìˆ˜
# ------------------------
def safe_decode(tokenizer, ids):
    try:
        # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ID í•„í„°ë§
        vocab_size = tokenizer.get_piece_size()
        valid_ids = [id for id in ids if id < vocab_size]

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if not valid_ids:
            return "<ë¹ˆ ì‹œí€€ìŠ¤>"

        return tokenizer.decode(valid_ids)
    except Exception as e:
        return f"ë””ì½”ë”© ì˜¤ë¥˜: {e}"


# ------------------------
# ê°€ì¥ ìµœì‹ ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ path ë°˜í™˜
# ------------------------
def get_latest_checkpoint(checkpoint_dir):
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
    if not checkpoints:
        raise FileNotFoundError("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
    return os.path.join(checkpoint_dir, latest_checkpoint)

# ------------------------
# 1. Luna ëª¨ë¸ ì •ì˜
# ------------------------
class LunaModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_attention_heads=8):
        super().__init__()
        self.encoder = LunaTransformerEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            num_layers=num_layers,
            num_attention_heads=num_attention_heads,
            d_ff=2048,
            dropout_p=0.2,
            project_embedding_length=32,
            max_length=1024
        )
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, input_lengths):
        # ì¸ì½”ë”ë¥¼ í†µê³¼
        encoder_outputs = self.encoder(input_ids, input_lengths)
        # ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ í†µê³¼
        logits = self.lm_head(encoder_outputs)
        return logits

# ------------------------
# 2. í•™ìŠµ ì„¤ì •
# ------------------------
def label_smoothed_loss(pred, target, epsilon=0.1):
    n_class = pred.size(-1)
    log_probs = F.log_softmax(pred, dim=-1)
    one_hot = F.one_hot(target, num_classes=n_class).float()
    smoothed_target = (1 - epsilon) * one_hot + epsilon / n_class
    loss = -(smoothed_target * log_probs).sum(dim=-1)
    return loss.mean()

def train():
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    BATCH_SIZE = 32
    EPOCHS = 50
    LEARNING_RATE = 0.0001
    D_MODEL = 512
    NUM_HEADS = 8
    NUM_LAYERS = 6
    D_FF = 2048
    MAX_SEQ_LENGTH = 128
    DROPOUT = 0.2
    VOCAB_SIZE = 16000
    max_length = 128
    PAD_TOKEN_ID = 0    # íŒ¨ë”© í† í° ID
    BOS_TOKEN_ID = 2    # ì‹œì‘ í† í° ID( <s> or [BOS] ) 

    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •
    transformer_path = "/workspace/transformer"
    train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
    val_path = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')
    tokenizer = SentencePieceTokenizer(train_path, vocab_size=VOCAB_SIZE, max_length=max_length).tokenizer
    dataset = SpellingDataset(train_path, val_path, tokenizer, max_length)
    train_loader = DataLoader(dataset.train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    checkpoints = [f for f in os.listdir(f"{drive_path}/transformer/checkpoints") if f.endswith('.pt')]
    
    # ëª¨ë¸ ê°ì²´ ìƒì„±
    model = LunaTransformer(
        vocab_size=VOCAB_SIZE,
        d_model=D_MODEL,
        num_layers=NUM_LAYERS,
        num_attention_heads=NUM_HEADS,
        d_ff = D_FF,
        dropout_p = DROPOUT,
        project_embedding_length = 32,
        max_length = MAX_SEQ_LENGTH
    )
    model = model.to(device)

    # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    scaler = GradScaler()  # FP16ì„ ìœ„í•œ Gradient Scaler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2, verbose=True
    )

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if not checkpoints:
        print("ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ.")
        latest_checknum = 0
    else:
        # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = get_latest_checkpoint(f"{drive_path}/transformer/checkpoints")
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

        # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
        latest_checkpoint = os.path.basename(checkpoint_path)
        latest_checknum = int(latest_checkpoint.split('_')[-1].split('.')[0])

        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        model = model.to(device)

    # í•™ìŠµ ë£¨í”„
    model.train()
    for epoch in range(latest_checknum, EPOCHS + latest_checknum):
        total_loss = 0
        total_edit_ratio = 0
        
        total_gold_edits = 0
        total_pred_edits = 0
        correct_edit_total = 0
        correct_tokens = 0
        total_tokens = 0
        
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS+latest_checknum}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            input_ids = batch['input_ids'].to(device)
            output_ids = batch['output_ids'].to(device)
            
            # input_lengths ê³„ì‚° (íŒ¨ë”© í† í° 0ì„ ì œì™¸í•œ ì‹¤ì œ ê¸¸ì´)
            input_lengths = (input_ids != PAD_TOKEN_ID).sum(dim=1).to(device)

            decoder_input = output_ids[:, :-1]  # Decoder ì…ë ¥
            target = output_ids[:, 1:]          # ì •ë‹µ
            optimizer.zero_grad()

            ### ë””ë²„ê¹…ìš© ì„ì‹œ ë³€ê²½ : autocast ì œê±°
            #with autocast():  # ìë™ í˜¼í•© ì •ë°€ë„ (FP16)      
            outputs = model(input_ids, input_lengths, decoder_input)
            outputs = outputs.view(-1, outputs.size(-1))
            target = target.contiguous().view(-1)
            """
            outputs = model(input_ids, input_lengths, output_ids)
            outputs = outputs.view(-1, outputs.size(-1))  # (batch_size * seq_len, vocab_size)
            target = output_ids.view(-1)  # (batch_size * seq_len)
            """
            if not torch.isfinite(outputs).all():
                print("[Debug]ë°°ì¹˜ë³„ target ë¹„-íŒ¨ë”© í† í° ìˆ˜:", (target != PAD_TOKEN_ID).sum(dim=1).tolist())
                print("[Debug]Decoder outputs shape:", outputs.shape)
                print("[Debug]Target shape:", target.shape)
                print("[DebugğŸš¨] outputs í…ì„œ ë‚´ NaN/Inf ì¡´ì¬!")
                print("ì˜ˆì‹œ ì¶œë ¥ (ì²« 5ê°œ):", outputs[0][:5])
                print("ìµœëŒ€ê°’:", outputs.max().item(), "ìµœì†Œê°’:", outputs.min().item(), "í‰ê· ê°’:", outputs.mean().item())

            #loss = criterion(outputs, target)
            loss = label_smoothed_loss(outputs, target)

            # FP16 í•™ìŠµì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ëœ ì—­ì „íŒŒ
            """
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            """
            # Gradient Clipping
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()


            # ìˆ˜ì • ë¹„ìœ¨ ê³„ì‚°
            edit_ratio = ((output_ids != input_ids) & (output_ids != 0)).float().mean().item()
            total_edit_ratio += edit_ratio

            total_loss += loss.item()
            if not torch.isfinite(loss):
                print(f"[DebugğŸš¨] ë¹„ì •ìƒ Loss ë°œìƒ: {loss.item()}")
                print("[Debug]í˜„ì¬ learning rate:", scheduler.optimizer.param_groups[0]['lr'])
            
            # ì˜ˆì¸¡
            pred_ids = outputs.argmax(dim=-1)
            pred_ids = pred_ids.view(output_ids.size(0), output_ids.size(1) - 1)

            # ìˆ˜ì • ë¹„ìœ¨
            edit_ratio = ((output_ids != input_ids) & (output_ids != PAD_TOKEN_ID)).float().mean().item()
            total_edit_ratio += edit_ratio

            # ì •í™•ë„ ê³„ì‚°ìš©
            target_2d = target.view(pred_ids.size(0), pred_ids.size(1))

            # ì •í™•ë„ ê³„ì‚°
            correct_tokens += ((pred_ids == target_2d) & (target_2d != PAD_TOKEN_ID)).sum().item()
            total_tokens += (target_2d != PAD_TOKEN_ID).sum().item()

            # í¸ì§‘ ì •í™•ë„
            gold_edits = ((output_ids[:, 1:] != input_ids[:, 1:]) & (output_ids[:, 1:] != PAD_TOKEN_ID))
            pred_edits = ((pred_ids != input_ids[:, 1:]) & (pred_ids != PAD_TOKEN_ID))
            correct_edits = ((pred_ids == output_ids[:, 1:]) & gold_edits)

            total_gold_edits += gold_edits.sum().item()
            total_pred_edits += pred_edits.sum().item()
            correct_edit_total += correct_edits.sum().item()
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                #'edit_ratio': f'{edit_ratio:.2f}'
            })
            
            # ìƒ˜í”Œ ì¶œë ¥ (ê° ì—í¬í¬ë§ˆë‹¤ 5ê°œ)
            if batch_idx < 5:
                try:
                    input_text = safe_decode(tokenizer, input_ids[0].cpu().tolist())
                    output_text = safe_decode(tokenizer, output_ids[0].cpu().tolist())
                    pred_ids_with_pad = [BOS_TOKEN_ID] + pred_ids[0].cpu().tolist()
                    pred_text = safe_decode(tokenizer, pred_ids_with_pad)

                    # === ì²« í† í° ë¹„êµ ë””ë²„ê·¸ ì¶”ê°€ ===
                    pred_first = safe_decode(tokenizer, [pred_ids[0, 0].item()])
                    gold_first = safe_decode(tokenizer, [output_ids[0, 1].item()])  # [BOS] ë‹¤ìŒ í† í°
                    print(f"\t> ì²« í† í° ë¹„êµ | ì˜ˆì¸¡: {pred_first} / ì •ë‹µ: {gold_first}")
                    # =============================

                    # ìƒ˜í”Œ ì¶œë ¥
                    print(f"\n\tìƒ˜í”Œ {batch_idx+1}:")
                    print(f"\t> ì…ë ¥: {input_text}")
                    print(f"\t> ì˜ˆì¸¡: {pred_text}")
                    print(f"\t> ì •ë‹µ: {output_text}")

                except Exception as e:
                    print(f"[ErrorğŸš¨] ìƒ˜í”Œ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        avg_loss = total_loss / len(train_loader)
        avg_edit_ratio = total_edit_ratio / len(train_loader)
        
        token_acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0

        precision = correct_edit_total / total_pred_edits if total_pred_edits > 0 else 0.0
        recall = correct_edit_total / total_gold_edits if total_gold_edits > 0 else 0.0
        beta = 0.5
        if precision + recall > 0:
            f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)
        else:
            f0_5 = 0.0

        print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}, Avg Edit Ratio: {avg_edit_ratio:.4f}")
        print(f"   Token Acc: {token_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")
        #print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Average Edit Ratio: {avg_edit_ratio:.4f}")
        
        with open(f"{transformer_path}\\epoch_metrics.csv", mode="a", newline="") as file:
            writer = csv.writer(file)
            if epoch == 0:
                writer.writerow(["epoch", "loss", "edit_ratio", "token_acc", "precision", "recall", "f0.5"])
            writer.writerow([
                epoch + 1,
                avg_loss,
                avg_edit_ratio,
                token_acc,
                precision,
                recall,
                f0_5
            ])

        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step(avg_loss)

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if((epoch + 1) % 5 == 0):
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                #'edit_ratio': avg_edit_ratio,
                'tokenizer': tokenizer
            }
            torch.save(checkpoint, f"{drive_path}/transformer/checkpoints/luna_model_epoch_{epoch+1}.pt")

# ------------------------
# 3. í‰ê°€ ì„¤ì •
# ------------------------
def evaluate():
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # (1) ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶° ì„¤ì •í•  ë¶€ë¶„
    # - CHECKPOINT_DIR: ì²´í¬í¬ì¸íŠ¸(.pt) íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    # - transformer_path: ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ train ì½”ë“œì™€ ì¼ê´€ë˜ë„ë¡)
    # - í•˜ì´í¼íŒŒë¼ë¯¸í„°: VOCAB_SIZE, max_length, BATCH_SIZE, PAD_TOKEN_ID ë“±
    CHECKPOINT_DIR = f"{drive_path}/transformer/checkpoints"
    transformer_path = f"{drive_path}/transformer"
    VOCAB_SIZE = 16000
    max_length = 1024
    BATCH_SIZE = 8
    PAD_TOKEN_ID = 0

    # (3) ë°ì´í„°ì…‹ ë° DataLoader ì¤€ë¹„
    # ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤: SentencePieceTokenizer, SpellingDataset, LunaTransformer ë“±ì„ ì´ë¯¸ import/ì •ì˜í•œ ìƒíƒœì—¬ì•¼ í•¨
    # ì˜ˆì‹œ:
    # tokenizer = SentencePieceTokenizer(train_path, vocab_size=VOCAB_SIZE, max_length=max_length).tokenizer
    # dataset = SpellingDataset(train_path, val_path, tokenizer, max_length)
    # val_loader = DataLoader(dataset.val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    train_path = os.path.join(transformer_path, 'TrainData/combined_train_dataset.json')
    val_path   = os.path.join(transformer_path, 'ValidationData/combined_validation_dataset.json')

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (í™˜ê²½ì— ë§ì¶° êµ¬í˜„ì²´ ì‚¬ìš©)
    tokenizer = SentencePieceTokenizer(train_path, vocab_size=VOCAB_SIZE, max_length=max_length).tokenizer

    # ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±
    dataset = SpellingDataset(train_path, val_path, tokenizer, max_length)
    val_loader = DataLoader(dataset.val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # (4) ëª¨ë¸ ë° criterion ì¤€ë¹„
    # LunaTransformer ìƒì„±ì ì¸ìëŠ” train ì‹œì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨
    model = LunaTransformer(
        vocab_size=VOCAB_SIZE,
        d_model=512,           # train ì½”ë“œì— ë§ì¶° ì¡°ì •
        num_layers=6,
        num_attention_heads=8,
        d_ff=2048,
        dropout_p=0.1,
        project_embedding_length=32,
        max_length=1024        # train ì‹œ positional encoding ê¸¸ì´ ë“±ê³¼ ì¼ì¹˜
    )
    model = model.to(device)

    # ì†ì‹¤ í•¨ìˆ˜: íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ë„ë¡ ignore_index ì„¤ì •
    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)

    # (5) ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    ckpt_path = get_latest_checkpoint(CHECKPOINT_DIR)
    if ckpt_path is None:
        print(">> ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", CHECKPOINT_DIR)
    else:
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        # checkpointì— ì €ì¥ëœ í‚¤ê°€ 'model_state_dict'ì¼ ê²½ìš°:
        model.load_state_dict(checkpoint['model_state_dict'])
        # íŒŒì¼ ì´ë¦„ì—ì„œ ì—í¬í¬ ë²ˆí˜¸ ì¶”ì¶œ
        latest_checkpoint = os.path.basename(ckpt_path)
        epoch_num = int(latest_checkpoint.split('_')[-1].split('.')[0])
        print(f">> ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path} (epoch {epoch_num})")

        # (6) Validation í‰ê°€ ë£¨í”„
        model.eval()
        total_loss = 0.0
        total_tokens = 0
        correct_tokens = 0

        total_gold_edits = 0
        total_pred_edits = 0
        correct_edits = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids = batch['input_ids'].to(device)    # (batch, seq_len)
                output_ids = batch['output_ids'].to(device)  # (batch, seq_len)

                # input_lengths ê³„ì‚°: ëª¨ë¸ forwardì— í•„ìš”í•œ ê²½ìš°
                input_lengths = (input_ids != PAD_TOKEN_ID).sum(dim=1).to(device)  # (batch,)

                # forward: teacher-forcing í˜•íƒœë¡œ êµ¬í˜„ëœ ëª¨ë¸ì´ë¼ë©´
                logits = model(input_ids, input_lengths, output_ids)  
                # logits shape: (batch, seq_len, vocab_size)

                # ì˜ˆì¸¡
                pred_ids = logits.argmax(dim=-1)  # (batch, seq_len)

                # ì†ì‹¤ ê³„ì‚°
                logits_flat = logits.view(-1, logits.size(-1))    # (batch*seq_len, vocab_size)
                target_flat = output_ids.view(-1)                 # (batch*seq_len,)
                loss = criterion(logits_flat, target_flat)
                # í† í° ê°œìˆ˜: paddingì´ ì•„ë‹Œ ìœ„ì¹˜ë§Œ ì¹´ìš´íŠ¸
                non_pad_mask = target_flat != PAD_TOKEN_ID
                num_non_pad = non_pad_mask.sum().item()
                total_loss += loss.item() * num_non_pad
                total_tokens += num_non_pad

                # ì •í™•íˆ ì˜ˆì¸¡í•œ í† í° ìˆ˜
                correct_tokens += ((pred_ids == output_ids) & (output_ids != PAD_TOKEN_ID)).sum().item()

                # GEC í¸ì§‘ ì§€í‘œ: 
                # gold edit ìœ„ì¹˜: output_ids != input_ids, output_ids != PAD
                gold_edits = ((output_ids != input_ids) & (output_ids != PAD_TOKEN_ID))
                # pred edit ìœ„ì¹˜: pred_ids != input_ids, pred_ids != PAD
                pred_edits = ((pred_ids != input_ids) & (pred_ids != PAD_TOKEN_ID))
                correct_edits += ((pred_ids == output_ids) & gold_edits).sum().item()
                total_gold_edits += gold_edits.sum().item()
                total_pred_edits += pred_edits.sum().item()

        # (7) ì§€í‘œ ê³„ì‚° ë° ì¶œë ¥
        if total_tokens > 0:
            avg_loss = total_loss / total_tokens
            token_acc = correct_tokens / total_tokens
        else:
            avg_loss = float('nan')
            token_acc = float('nan')

        precision = correct_edits / total_pred_edits if total_pred_edits > 0 else 0.0
        recall = correct_edits / total_gold_edits if total_gold_edits > 0 else 0.0
        beta = 0.5
        if precision + recall > 0:
            f0_5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)
        else:
            f0_5 = 0.0

        print(f"\n>> Validation ê²°ê³¼ (epoch {epoch_num}):")
        print(f"   í‰ê·  ì†ì‹¤: {avg_loss:.4f}, í† í° ì •í™•ë„: {token_acc:.4f}")
        print(f"   Precision: {precision:.4f}, Recall: {recall:.4f}, F0.5: {f0_5:.4f}")

if __name__ == '__main__':
    train()
